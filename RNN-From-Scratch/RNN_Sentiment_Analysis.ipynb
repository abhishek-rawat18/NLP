{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MDS201802_NLP_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxhNvTRP0SRE",
        "colab_type": "text"
      },
      "source": [
        "#**SENTIMENT ANALYSIS USING RNN**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKrcLaD1Ibs3",
        "colab_type": "text"
      },
      "source": [
        "##Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GQ1hfYPWCWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUOto6IaWG94",
        "colab_type": "code",
        "outputId": "16922090-1325-4778-e547-98023e53d565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT9ebJ4FKex1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuaCtskXWIQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "tar=tarfile.open(\"/content/drive/My Drive/NLP HW 3/aclImdb_v1.tar.gz\", \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "#https://docs.python.org/3/library/tarfile.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETOY6RohWPS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "pos_path_train=\"/content/aclImdb/train/pos/\"\n",
        "pos_files_train=[f for f in listdir(pos_path_train)]\n",
        "neg_path_train=\"/content/aclImdb/train/neg/\"\n",
        "neg_files_train=[f for f in listdir(neg_path_train)]\n",
        "pos_path_test=\"/content/aclImdb/test/pos/\"\n",
        "pos_files_test=[f for f in listdir(pos_path_test)]\n",
        "neg_path_test=\"/content/aclImdb/test/neg/\"\n",
        "neg_files_test=[f for f in listdir(neg_path_test)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo5oAAh2WQx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pos_train is the list of positive comments in training data\n",
        "#same for neg.\n",
        "\n",
        "pos_train=[]\n",
        "for i in range(len(pos_files_train)):\n",
        "    file=open(\"/content/aclImdb/train/pos/\"+pos_files_train[i],\"r\", encoding='utf-8')\n",
        "    input=file.read()\n",
        "    pos_train.append(input)\n",
        "\n",
        "neg_train=[]\n",
        "for i in range(len(neg_files_train)):\n",
        "    file=open(\"/content/aclImdb/train/neg/\"+neg_files_train[i],\"r\", encoding='utf-8')\n",
        "    input=file.read()\n",
        "    neg_train.append(input)\n",
        "\n",
        "pos_test=[]\n",
        "for i in range(len(pos_files_test)):\n",
        "    file=open(\"/content/aclImdb/test/pos/\"+pos_files_test[i],\"r\", encoding='utf-8')\n",
        "    input=file.read()\n",
        "    pos_test.append(input)\n",
        "\n",
        "neg_test=[]\n",
        "for i in range(len(neg_files_test)):\n",
        "    file=open(\"/content/aclImdb/test/neg/\"+neg_files_test[i],\"r\", encoding='utf-8')\n",
        "    input=file.read()\n",
        "    neg_test.append(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UZlXuUdLoyG",
        "colab_type": "text"
      },
      "source": [
        "So we have our data as lists in pos_train, neg_train, pos_test, neg_test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S-CnXHvL99h",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUvOy4wxKK2s",
        "colab_type": "text"
      },
      "source": [
        "##Exploratory Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU1dtrR_s8WU",
        "colab_type": "code",
        "outputId": "d8135071-4479-4b07-b6c2-502b415c077c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(pos_train),len(neg_train),len(pos_test),len(neg_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500, 12500, 12500, 12500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGzMbNvEtQMU",
        "colab_type": "text"
      },
      "source": [
        "So we have eually divided training and test data sets with equal number of observations for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nwco1NstaCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now checking for review lengths in case of training and test data respectively\n",
        "sent_len=[]\n",
        "for sentence in pos_train:\n",
        "    sent_len.append(len(sentence.split()))\n",
        "for sentence in neg_train:\n",
        "    sent_len.append(len(sentence.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul_PMrL5uMsy",
        "colab_type": "code",
        "outputId": "c0910e0a-c560-456b-ef78-de80bc5c05be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(sent_len)\n",
        "plt.xlabel('Review Length',fontsize=15)\n",
        "plt.ylabel('Number of Reviews',fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A6hXlDA8wNTF",
        "colab": {}
      },
      "source": [
        "sent_len=[]\n",
        "for sentence in pos_test:\n",
        "    sent_len.append(len(sentence.split()))\n",
        "for sentence in neg_test:\n",
        "    sent_len.append(len(sentence.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f2ee7dc0-249b-4eb6-833f-a02039b31a14",
        "id": "WvIx3wV-wNTq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "plt.hist(sent_len)\n",
        "plt.xlabel('Review Length',fontsize=15)\n",
        "plt.ylabel('Number of Reviews',fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7BmVFfMwlPc",
        "colab_type": "text"
      },
      "source": [
        "Thus we observe that in the case of both training and test data, more than 50% reviews are of length less than 250. Some more in the range of 250-500. There are only very few reviews having length >500. Note that all of this is before preprocessing the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrumEfiBIrUf",
        "colab_type": "code",
        "outputId": "63eb5697-4ca2-438c-cc2a-e8a0e0297e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Lets take a look at one of the file names\n",
        "pos_files_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1269_8.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9cixy1SJD18",
        "colab_type": "text"
      },
      "source": [
        "As in the decription of the data, the file name is of the form \"uniqueID_score\". Where score is the rating given for the particular review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh3Fi4Y8luvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extracting the scores for positive and negative training data\n",
        "pos_train_scores=[int(i.split(\"_\")[1].split(\".\")[0]) for i in pos_files_train]\n",
        "neg_train_scores=[int(i.split(\"_\")[1].split(\".\")[0]) for i in neg_files_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kf8JcF9m8si",
        "colab_type": "code",
        "outputId": "269dcffa-ee0e-4fc7-d6ae-7cdbb0f07db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "set(pos_train_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{7, 8, 9, 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkIMIh6ZJbVg",
        "colab_type": "text"
      },
      "source": [
        "Positive train data contains scores from 7 to 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDHFBoAHsMh7",
        "colab_type": "code",
        "outputId": "e9b78257-1c36-467a-c319-238a200e56b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pos_train_scores.count(7),pos_train_scores.count(8),pos_train_scores.count(9),pos_train_scores.count(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2496, 3009, 2263, 4732)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrPturv-Jq8m",
        "colab_type": "text"
      },
      "source": [
        "The data for positive case is good as we have more of score 10 reviews compared to the other smaller scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW6uuaSNryXB",
        "colab_type": "code",
        "outputId": "b452392e-3ec5-48a2-c4fb-4b288ce38d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "set(neg_train_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1, 2, 3, 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu4Ofav1J3Uz",
        "colab_type": "text"
      },
      "source": [
        "Negative train data contains scores from 1 to 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHBRoOTyoFpi",
        "colab_type": "code",
        "outputId": "6dc8901f-8b6d-45f3-fe89-4d9aa783f1e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "neg_train_scores.count(1),neg_train_scores.count(2),neg_train_scores.count(3),neg_train_scores.count(4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5100, 2284, 2420, 2696)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2Tqt8xCJ--c",
        "colab_type": "text"
      },
      "source": [
        "The data for negative case is also good as we have more of score 1 reviews compared to the other greater scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh204tAbJnVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extracting the scores for positive and negative test data\n",
        "pos_test_scores=[int(i.split(\"_\")[1].split(\".\")[0]) for i in pos_files_test]\n",
        "neg_test_scores=[int(i.split(\"_\")[1].split(\".\")[0]) for i in neg_files_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cc88db2e-05b2-447a-ec96-abe426553db8",
        "id": "xG2Eb3hpLKp1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "set(pos_test_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{7, 8, 9, 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "35fc1f94-e6c7-472b-c65a-a6a571fd094c",
        "id": "7hGoDJThLKqu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pos_test_scores.count(7),pos_test_scores.count(8),pos_test_scores.count(9),pos_test_scores.count(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2307, 2850, 2344, 4999)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "513799b6-9da6-42f7-9700-dc033854040d",
        "id": "tR0-87S1LKrS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "set(neg_test_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1, 2, 3, 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e986cf38-848c-40de-a79c-dbd6112f9108",
        "id": "fGJbGkMZLKru",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "neg_test_scores.count(1),neg_test_scores.count(2),neg_test_scores.count(3),neg_test_scores.count(4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5022, 2302, 2541, 2635)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T311Bm4kLUt8",
        "colab_type": "text"
      },
      "source": [
        "We have the similar conclusion for the test data also as the training one. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBYQn9sd9WKp",
        "colab_type": "code",
        "outputId": "9a3c00ca-58fc-472b-c414-152cc6bd20e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#Now lets take a look at some of the text.\n",
        "pos_train[10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It\\'s hard to imagine a director capable of such godawful crap as \\'Notting Hill\\' pulling off something as sensitive and as attractive as this, but well, here\\'s the evidence and it\\'s quite compelling. Several have alluded to TV drama, and yes, this does have a seventies Play for Today feel at times, but is always a cut above, mainly I think owing to some quite superlative acting from Anne Reid and to a fine script which shadow-boxes with cliché without ever getting one on the nose, except maybe right at the end. (I didn\\'t like either the tracking shot of indifferent goodbyes through the hallway, nor the oh-what-a-beautiful-morning final scene: she deserved a more studied finale than that I think, after all that hard work. The slippers business was a bit OTT too, on reflection).<br /><br />What I mean about avoiding cliché: well, I for one had a sinking expectation that the \"mature\" man May\\'s daughter tries to set her up with would be cast in 2 dimensions as a repulsive old bore, so as to point the contrast more painfully with the attractive, virile young geezer he is unwittingly competing with. Instead, we get an unexpectedly subtle and sympathetic cameo of a lonely, clumsy, not entirely unlikeable and very human fellow, who nevertheless doesn\\'t have much of a clue about entertaining a woman. It was around that point I started to sit up and pay more attention. Here was a script that let the actors breathe and do something interesting with fairly minor parts. Almost Mike Leigh in that respect (minus the contrived catharses that the latter inexplicably goes in for).<br /><br />And of course I was, as everyone probably was, dumbfounded by what Anne Reid does with her character and with her body. She\\'s /not/ \"the repressed, dutiful housewife discovering herself for the first time\", this is far too simplistic for the character we have. Again and again there are allusions to her having been a \"bad housewife\", not to mention that thing she does with trays, trying to look nurturing and comely and only succeeding in looking awkward. The daughter accuses her of having \"sat in front of the TV all day\" instead of, well, whatever her motherly duties might be presumed to have been: she has no answer. She never was a model wife and mother, at least not to herself - that\\'s where a lot of the poignancy comes from, the sense of someone having wasted a life trying to fulfil a role she simply wasn\\'t good at, ever.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2wZX9xEL4xF",
        "colab_type": "text"
      },
      "source": [
        "We need to remove characters like \"\\\\\" and ensure that no space is added when we remove them. We will also replace the apostrophe sign without space because later I will use GLOVE embeddings to train and it has words like it's without the apostrophe (i.e. its)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8lmcYKgMbD_",
        "colab_type": "code",
        "outputId": "1fe83327-e5cd-4d36-c368-b98e49c1710a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "pos_train[1000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this movie is practically impossible to describe. the alternate title \"Don\\'t Look Up\" is a lot more descriptive. Like most Japanese cinema, the story is not as linear as American. The story revolves around a director who is filming a story about a ww2 deserter. The set is haunted(?) by an actress who died(?) during the filming of a tv show back in the 60s. the director is the ONLY one who saw this show. if you have seen Ringu (the director Hideo Nakata is the same) and liked it, you\\'ll like ghost actress. i loved ghost actress a lot more than ringu. a truly scary and disturbing movie. a 10!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR996NOWMpct",
        "colab_type": "text"
      },
      "source": [
        "Other non important characters like \")\" can be removed and replaced by space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoBgUdggNTne",
        "colab_type": "code",
        "outputId": "fbedc7d1-443d-42a2-e143-f5b80c4759bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "neg_train[90]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In a way this is the disaster Fellini has been working towards all his life. The line between absurd masterpiece and free association bullshit is very small, and what category a film will ultimately fit in will often just depend on personal feelings. That said, \"Casanova\" left me in cold admiration for its sets and little more that cannot be summed up more adequately by Bukowski: <br /><br />\"Casanova died too, just an old guy with a big cock and a long tongue and no guts at all. to say that he lived well is true; to say I could spit on his grave without feeling is also true. the ladies usually go for the biggest fool they can find; that is why the human race stands where it does today: we have bred the clever and lasting Casanovas, all hollow inside, like the Easter bunnies we foster upon our poor children.\" <br /><br />As far as I could make it out, this is the position Fellini takes regarding his subject; granted, with more empathy, but disgusted nonetheless.<br /><br />Casanova\\'s environment is made from decay and incestuous behavior, themes Fellini dealt with more pointedly in \"Satyricon\". The succession of plot is characteristic of soft porn, just without the coherence; and Donald Sutherland is ugly and slimy to the point of distraction.<br /><br />Yet, there might just be a point in portraying Casanova as an unsightly fool. And I challenge anybody to formulate this point without being obvious; Fellini couldn\\'t. More than ever he seems here like a dirty old man - a maestro, for sure, but one whose impulses satisfy himself more than anybody else. I find it hard imagine an audience who enjoys this film. It was a story not worth telling.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLLM-tzjNaPQ",
        "colab_type": "text"
      },
      "source": [
        "We also need to remove \"\\<br />\\<br />\" sort of stuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVQeIayHN4HI",
        "colab_type": "text"
      },
      "source": [
        "Also other possibility that I observed was text like \"xyz......pqr\". This makes it necessary to ensure that . is replaced by space otherwise both words might get combined!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-7PdhHUPL5t",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rMsCjfiPOFp",
        "colab_type": "text"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-REea_EWqCTg",
        "colab_type": "code",
        "outputId": "e86b1e0a-f39b-4022-b33e-1219da2bf1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI7rMxXoPH4B",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing the data to handle all the points discussed above. Also eliminating the stop words. Ignoring Lemmatization as GLOVE already has vectors for those words (Ex: bright brighter brightest). Note that I tried the complete thing with Lemmatization also and the results were almost the same. Also I tried keeping the stopwords thinking that they might lead to some symantic info but the results were comparitively worse than removing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXgllX49pahm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=set(stopwords.words('english')) \n",
        "\n",
        "def preprocessing(lst):\n",
        "    for i in range(len(lst)):\n",
        "        temp=re.sub(r'[^a-zA-Z\\'\\\\ ]+',' ',lst[i]) #Only leaving behind the alphabetical characters space and ' and \\ marks.\n",
        "        temp=re.sub(r'[\\'\\\\]+','',temp) #Taking care of ' and \\ marks\n",
        "        temp=re.sub(r' br ','',temp) #Taking care of <br /><br />\n",
        "        temp=temp.lower() #Converting the entire string to lower case.           \n",
        "        words=word_tokenize(temp)\n",
        "        words=[w for w in words if (w not in stop_words and len(w)>1)] #Removing stop words and single letter words\n",
        "        temp=' '.join(words)\n",
        "        lst[i]=temp\n",
        "    return lst\n",
        "\n",
        "pos_train=preprocessing(pos_train)\n",
        "neg_train=preprocessing(neg_train)\n",
        "\n",
        "pos_test=preprocessing(pos_test)\n",
        "neg_test=preprocessing(neg_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCx0E_WMQB3w",
        "colab_type": "text"
      },
      "source": [
        "So now we have the preprocessed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCWDkMWsQdVc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80gUWsMJQF6D",
        "colab_type": "text"
      },
      "source": [
        "##Designing The RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNKigSphC_bB",
        "colab_type": "text"
      },
      "source": [
        "The structure of the RNN I've used is shown below:-\n",
        "\n",
        "$x_1,x_2,...x_n$ --> Inputs\n",
        "\n",
        "$h_0,h_1,...h_n$ --> Hidden Layers\n",
        "\n",
        "$s_i=tanh(h_i)$\n",
        "\n",
        "$\\hat{y}$ --> Expected Output\n",
        "\n",
        "$W_{hy}, W_{xh}, W_{hh}$ --> Weights\n",
        "\n",
        "$b_h, b_y$ --> Biases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsnf4sJxB0OR",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1v1_sY6zpuAcb-VqjEB4aPG5f5p_FgRsK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggxFe0liEqfZ",
        "colab_type": "text"
      },
      "source": [
        "(I've made the Image using Libre Office)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxY7lG0Yeyk1",
        "colab_type": "text"
      },
      "source": [
        "The detailed explaination and derivation for forward and backward propagation can be accesssed via the following link to my file: \n",
        "https://drive.google.com/file/d/1EhNB83ZeilWQjIFc4le_S6uw1fFeItw1/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tY7C3QbW_SIh",
        "colab": {}
      },
      "source": [
        "#The function names below stand for the work the function does\n",
        "\n",
        "def softmax(x):\n",
        "    z=np.exp(x)\n",
        "    s=np.sum(z)\n",
        "    return z/s\n",
        "\n",
        "def tanh(x):\n",
        "    return((2.0/(1+np.exp(-2*x)))-1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFvpMyAwSGdv",
        "colab_type": "text"
      },
      "source": [
        "Forward and Backward propagation algorithm below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udOAcDvHau9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(inputs,h): #inputs contains the various seq of inputs in proper format\n",
        "\n",
        "    S_initial=np.zeros((h,1)) #the random S (say it corresponds to time 0 ...the other S start from time 1 and so on)\n",
        "    S_saved={0:S_initial} #A dict with S corresponding to the times 0,1,2,... as the key\n",
        "    #Note: We'll be saving the S at each time step, as it will be req during backprop along with the inputs \n",
        "    #(need not return the inputs here as they will be present in the function from where we called forward_prop)\n",
        "\n",
        "    for i,X in enumerate(inputs):\n",
        "        S_prev=S_saved[i]\n",
        "        S=tanh(np.dot(Wxh.T,X)+np.dot(Whh.T,S_prev)+bh)\n",
        "        S_saved[i+1]=S #Saving the S computed for next time step in the dict\n",
        "\n",
        "    #At the last one, compute the output\n",
        "    y=np.dot(Why.T,S_saved[len(inputs)])+by #Here, the S will be the last computed one\n",
        "    return S_saved,y\n",
        "\n",
        "def backward_prop(S_saved,inputs_saved,y,correct_class,alpha): #alpha=learning rate\n",
        "\n",
        "    global Why,by,Wxh,Whh,bh\n",
        "\n",
        "    #Initial zero values for these derivatives of weights and biases (will be updated below)\n",
        "    dL_dWhy=np.zeros(Why.shape)\n",
        "    dL_dby=np.zeros(by.shape)\n",
        "    dL_dWxh=np.zeros(Wxh.shape)\n",
        "    dL_dWhh=np.zeros(Whh.shape)\n",
        "    dL_dbh=np.zeros(bh.shape)\n",
        "\n",
        "    p=softmax(y)\n",
        "    dL_dy=p[:]\n",
        "    dL_dy[correct_class]=dL_dy[correct_class]-1\n",
        "    #dL/dy = pi if i <> c else = pi-1 if i = c\n",
        "\n",
        "    n=len(inputs_saved)\n",
        "\n",
        "    dL_dWhy=np.dot(S_saved[n],dL_dy.T)\n",
        "    #dL/dWhy = dL/dy * Sn\n",
        "    \n",
        "    dL_dby=dL_dy\n",
        "    #dL/dby=dL/dy\n",
        "\n",
        "    #The above ones were easy. Difficult ones below\n",
        "\n",
        "    dL_dS=np.dot(Why,dL_dy) #For nth S(i.e. the last) ...dL/dSn=dL/dy*dy/dSn ...dy/dSn=Why    \n",
        "    \n",
        "    for i in range(n,0,-1): #In the reverse order\n",
        "          \n",
        "        common_term=1-S_saved[i]**2 #This term will appear in all the remaining derivatives\n",
        "\n",
        "        common_term=common_term*dL_dS\n",
        "\n",
        "        dL_dWxh+=np.dot(inputs_saved[i-1],common_term.T)\n",
        "        #dL/dWxh=sum(dL/dS[t] * [ (1-S[t])^2 * Xt ])\n",
        "\n",
        "        dL_dWhh+=np.dot(S_saved[i-1],common_term.T)\n",
        "        #dL/dWhh=sum(dL/dS[t] * [ (1-S[t])^2 * S[t-1] ])\n",
        "\n",
        "        dL_dbh+=common_term\n",
        "        #dL/dbh=sum(dL/dS[t] * (1-S[t])^2)\n",
        "\n",
        "        dL_dS=np.dot(Whh,common_term)\n",
        "        #dL/dS[i-1]=dL/dS[i] * [ (1-S[i])^2 * Whh ]\n",
        "\n",
        "    #Gradient clipping to handle exploding gradients.\n",
        "    np.clip(dL_dWhy, -1, 1, out=dL_dWhy)\n",
        "    #Updating weights\n",
        "    Why-=alpha*dL_dWhy\n",
        "\n",
        "    np.clip(dL_dby, -1, 1, out=dL_dby)\n",
        "    by-=alpha*dL_dby\n",
        "        \n",
        "    np.clip(dL_dWxh, -1, 1, out=dL_dWxh)\n",
        "    Wxh-=alpha*dL_dWxh\n",
        "        \n",
        "    np.clip(dL_dWhh, -1, 1, out=dL_dWhh)\n",
        "    Whh-=alpha*dL_dWhh\n",
        "    \n",
        "    np.clip(dL_dbh, -1, 1, out=dL_dbh)\n",
        "    bh-=alpha*dL_dbh\n",
        "\n",
        "    #So weights and biases updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTNuDaU5SLKo",
        "colab_type": "text"
      },
      "source": [
        "Using GloVe to pass inputs to the RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mJgluUdQXTWc",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile \n",
        "file_name=\"/content/drive/My Drive/NLP HW 3/glove.6B.300d.zip\"\n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "    zip.extractall() \n",
        "\n",
        "#Creating a GLOVE embeddings dictionary\n",
        "#https://github.com/stanfordnlp/GloVe\n",
        "#Taken the Wiki 6B tokens pre trained word vectors of 300 dimension\n",
        "\n",
        "embeddings_dict={}\n",
        "with open(\"glove.6B.300d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values=line.split()\n",
        "        word=values[0]\n",
        "        vector=np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word]=vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu7kSvCiQ-Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createInputs(text):\n",
        "  #Returns the GLOVE vectors corresponding to each of the words in the text \n",
        "  inputs=[]\n",
        "  for w in text.split(' '):\n",
        "    if(w in embeddings_dict):\n",
        "        v=embeddings_dict[w]\n",
        "        v.shape=[300,1]\n",
        "        inputs.append(v)\n",
        "  return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHLvTaCSUNng",
        "colab_type": "text"
      },
      "source": [
        "I had one more idea of taking bigrams for passing as input. I took the GloVe embeddings as the average for the bigrams. This gave a bit better result sometimes. But the difference was very small. So we will go with the normal way of passing input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMciaMG072BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(h,epochs,alpha):\n",
        "    #Initial Values of weights and biases\n",
        "    global Why,by,Wxh,Whh,bh\n",
        "    i= 300 # i = input layer size\n",
        "    o= 2 # op = output layer size\n",
        "    # h = hidden layer size\n",
        "\n",
        "\n",
        "    #We can make use of Xaviers inititalization technique for weights \n",
        "    #but partially based on that and intuitively we can set the initial weights as follows\n",
        "    #because most of our input length would be around 250\n",
        "    Wxh=np.random.uniform(-0.9,0.9,(i,h))/250\n",
        "    Why=np.random.uniform(-0.9,0.9,(h,o))/250\n",
        "    Whh=np.random.uniform(-0.9,0.9,(h,h))/250\n",
        "    #We initialize the bias values to zeroes as initially we don't have any bias\n",
        "    bh=np.zeros((h,1))\n",
        "    by=np.zeros((o,1))\n",
        "\n",
        "    len_train=len(pos_train)+len(neg_train)\n",
        "\n",
        "    #epochs=20\n",
        "    #alpha=0.01\n",
        "\n",
        "    #Early stopping parameters\n",
        "    #To make the training stop early based on some criterion(if required)\n",
        "    temp=100\n",
        "    f=0\n",
        "\n",
        "    print(\"Training has begun...\\n\")\n",
        "\n",
        "    #Learning i.e forward and backward propagation invoked below\n",
        "    for i in range(0,epochs):\n",
        "        \n",
        "        loss=0\n",
        "        acc_count=0\n",
        "\n",
        "        for j in range(len(pos_train)):\n",
        "\n",
        "            inputs=createInputs(pos_train[j])\n",
        "\n",
        "            S_saved,y=forward_prop(inputs,h) #inputs contains the various seq of inputs in proper format\n",
        "\n",
        "            correct_class=1\n",
        "\n",
        "            loss=-np.log(softmax(y)[correct_class]) #Cross entropy loss\n",
        "\n",
        "            if(np.argmax(softmax(y))==1):\n",
        "                acc_count+=1\n",
        "\n",
        "            backward_prop(S_saved,inputs,y,correct_class,alpha) #alpha=learning rate\n",
        "\n",
        "\n",
        "            inputs=createInputs(neg_train[j])\n",
        "\n",
        "            S_saved,y=forward_prop(inputs,h) #inputs contains the various seq of inputs in proper format\n",
        "\n",
        "            correct_class=0\n",
        "\n",
        "            loss=-np.log(softmax(y)[correct_class])\n",
        "\n",
        "            if(np.argmax(softmax(y))==0):\n",
        "                acc_count+=1\n",
        "\n",
        "            backward_prop(S_saved,inputs,y,correct_class,alpha) #alpha=learning rate\n",
        "        \n",
        "        #Early Stopping\n",
        "        if(loss>temp or np.abs(loss-temp)<0.00001):\n",
        "            f+=1\n",
        "        else:\n",
        "            f=0 #to reset counter if it again decreases\n",
        "        \n",
        "        temp=loss\n",
        "        \n",
        "        print(\"Loss: \",loss)\n",
        "\n",
        "        acc=acc_count*100/len_train\n",
        "\n",
        "        print(\"Accuracy: \",acc)\n",
        "        print(\"\\n\")\n",
        "\n",
        "        #In case loss increases for 3 continuous epochs or there is hardly any change then stop\n",
        "        if(f==3):\n",
        "            break\n",
        "    \n",
        "    print(\"\\nTesting has begun...\\n\")\n",
        "\n",
        "    test(h)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwF1ZSvs-i_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(h):\n",
        "  \n",
        "    count=0 #To store count of correct classifications\n",
        "\n",
        "    for j in range(len(pos_test)):\n",
        "\n",
        "            inputs=createInputs(pos_test[j])\n",
        "\n",
        "            S_saved,y=forward_prop(inputs,h) #inputs contains the various seq of inputs in proper format\n",
        "\n",
        "            if(np.argmax(softmax(y))==1):\n",
        "                count+=1\n",
        "\n",
        "    for j in range(len(neg_test)):\n",
        "\n",
        "            inputs=createInputs(neg_test[j])\n",
        "\n",
        "            S_saved,y=forward_prop(inputs,h) #inputs contains the various seq of inputs in proper format\n",
        "\n",
        "            if(np.argmax(softmax(y))==0):\n",
        "                count+=1\n",
        "\n",
        "    print(\"Test Accuracy = \",count*100/(len(pos_test)+len(neg_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwlFWuTtTlz_",
        "colab_type": "text"
      },
      "source": [
        "The paramters for training are h, epochs, alpha i.e the hidden layer size, epochs, and the learning rate respectively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQmfn0HETxy8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9iF5wDtTy6p",
        "colab_type": "text"
      },
      "source": [
        "##Training and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCO1CdTPT5We",
        "colab_type": "text"
      },
      "source": [
        "After a lot of training and testing, I've come up with the following hyperparameters to give the best results. A few more results are shown below that. Note that I've tried the training and testing for a lot more combinations of hyperparameters than the one below. But I've only shown a few here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bBsjMODcNfY",
        "colab_type": "code",
        "outputId": "36f9411a-bfef-4d87-c192-edd337369391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(h=20,epochs=30,alpha=0.005)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69325396]\n",
            "Accuracy:  0.108\n",
            "\n",
            "\n",
            "Loss:  [0.6931828]\n",
            "Accuracy:  28.508\n",
            "\n",
            "\n",
            "Loss:  [0.69180188]\n",
            "Accuracy:  56.704\n",
            "\n",
            "\n",
            "Loss:  [0.62917666]\n",
            "Accuracy:  59.532\n",
            "\n",
            "\n",
            "Loss:  [0.56592648]\n",
            "Accuracy:  70.512\n",
            "\n",
            "\n",
            "Loss:  [0.26132231]\n",
            "Accuracy:  77.648\n",
            "\n",
            "\n",
            "Loss:  [0.34904676]\n",
            "Accuracy:  78.132\n",
            "\n",
            "\n",
            "Loss:  [0.31843654]\n",
            "Accuracy:  78.052\n",
            "\n",
            "\n",
            "Loss:  [0.28516991]\n",
            "Accuracy:  78.676\n",
            "\n",
            "\n",
            "Loss:  [0.26267135]\n",
            "Accuracy:  78.524\n",
            "\n",
            "\n",
            "Loss:  [0.30058682]\n",
            "Accuracy:  78.704\n",
            "\n",
            "\n",
            "Loss:  [0.26431629]\n",
            "Accuracy:  78.92\n",
            "\n",
            "\n",
            "Loss:  [0.21115869]\n",
            "Accuracy:  79.088\n",
            "\n",
            "\n",
            "Loss:  [0.25152723]\n",
            "Accuracy:  79.376\n",
            "\n",
            "\n",
            "Loss:  [0.22107362]\n",
            "Accuracy:  79.532\n",
            "\n",
            "\n",
            "Loss:  [0.24544472]\n",
            "Accuracy:  79.78\n",
            "\n",
            "\n",
            "Loss:  [0.17260871]\n",
            "Accuracy:  79.724\n",
            "\n",
            "\n",
            "Loss:  [0.16356832]\n",
            "Accuracy:  79.648\n",
            "\n",
            "\n",
            "Loss:  [0.14281092]\n",
            "Accuracy:  79.88\n",
            "\n",
            "\n",
            "Loss:  [0.11966811]\n",
            "Accuracy:  80.0\n",
            "\n",
            "\n",
            "Loss:  [0.14883818]\n",
            "Accuracy:  80.096\n",
            "\n",
            "\n",
            "Loss:  [0.12968356]\n",
            "Accuracy:  80.216\n",
            "\n",
            "\n",
            "Loss:  [0.12777341]\n",
            "Accuracy:  80.216\n",
            "\n",
            "\n",
            "Loss:  [0.12951464]\n",
            "Accuracy:  80.344\n",
            "\n",
            "\n",
            "Loss:  [0.12365823]\n",
            "Accuracy:  80.728\n",
            "\n",
            "\n",
            "Loss:  [0.13002132]\n",
            "Accuracy:  80.644\n",
            "\n",
            "\n",
            "Loss:  [0.11930099]\n",
            "Accuracy:  80.864\n",
            "\n",
            "\n",
            "Loss:  [0.13612484]\n",
            "Accuracy:  80.692\n",
            "\n",
            "\n",
            "Loss:  [0.156781]\n",
            "Accuracy:  80.536\n",
            "\n",
            "\n",
            "Loss:  [0.13193501]\n",
            "Accuracy:  80.732\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  79.828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbykYH18cbkl",
        "colab_type": "text"
      },
      "source": [
        "We get a training accuracy of 80.7% and a test accuracy of 79.8%. (You might have to scroll down the above output to view the test accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3c477b55-08a5-4b4b-eeb8-c89d5444feb6",
        "id": "PE0MF7zEcyRG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Trying to do grid search to find comparitively better values of h and alpha\n",
        "#Note that these involved stricter early stopping criteria than the one above\n",
        "\n",
        "for h in [5,10,20,50]:\n",
        "    for alpha in [0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]:\n",
        "        \n",
        "        print(\"h = \",h,\"\\nalpha = \",alpha,\"\\n\")\n",
        "        train(h=h,epochs=50,alpha=alpha)\n",
        "        print(\"\\n _________________________________________________________ \\n\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h =  5 \n",
            "alpha =  0.1 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.71139843]\n",
            "Accuracy:  52.376\n",
            "\n",
            "\n",
            "Loss:  [1.7738276]\n",
            "Accuracy:  58.808\n",
            "\n",
            "\n",
            "Loss:  [1.81867172]\n",
            "Accuracy:  60.408\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  65.812\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.05 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [1.0283157]\n",
            "Accuracy:  59.928\n",
            "\n",
            "\n",
            "Loss:  [1.41743448]\n",
            "Accuracy:  65.788\n",
            "\n",
            "\n",
            "Loss:  [1.05428875]\n",
            "Accuracy:  68.152\n",
            "\n",
            "\n",
            "Loss:  [0.28443943]\n",
            "Accuracy:  67.708\n",
            "\n",
            "\n",
            "Loss:  [1.13051912]\n",
            "Accuracy:  68.624\n",
            "\n",
            "\n",
            "Loss:  [1.11396855]\n",
            "Accuracy:  69.388\n",
            "\n",
            "\n",
            "Loss:  [1.17267521]\n",
            "Accuracy:  69.56\n",
            "\n",
            "\n",
            "Loss:  [0.11640601]\n",
            "Accuracy:  69.224\n",
            "\n",
            "\n",
            "Loss:  [1.69979339]\n",
            "Accuracy:  70.848\n",
            "\n",
            "\n",
            "Loss:  [2.03044222]\n",
            "Accuracy:  71.392\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  72.84\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.01 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.38867429]\n",
            "Accuracy:  59.136\n",
            "\n",
            "\n",
            "Loss:  [0.34158998]\n",
            "Accuracy:  72.88\n",
            "\n",
            "\n",
            "Loss:  [0.24058686]\n",
            "Accuracy:  75.076\n",
            "\n",
            "\n",
            "Loss:  [1.14361523]\n",
            "Accuracy:  76.0\n",
            "\n",
            "\n",
            "Loss:  [0.21439271]\n",
            "Accuracy:  75.88\n",
            "\n",
            "\n",
            "Loss:  [0.17138099]\n",
            "Accuracy:  76.74\n",
            "\n",
            "\n",
            "Loss:  [1.45865115]\n",
            "Accuracy:  77.02\n",
            "\n",
            "\n",
            "Loss:  [0.11964053]\n",
            "Accuracy:  77.696\n",
            "\n",
            "\n",
            "Loss:  [0.18914325]\n",
            "Accuracy:  77.08\n",
            "\n",
            "\n",
            "Loss:  [0.24052294]\n",
            "Accuracy:  77.348\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  77.192\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [1.28447278]\n",
            "Accuracy:  48.36\n",
            "\n",
            "\n",
            "Loss:  [1.26516913]\n",
            "Accuracy:  73.328\n",
            "\n",
            "\n",
            "Loss:  [1.40151603]\n",
            "Accuracy:  75.244\n",
            "\n",
            "\n",
            "Loss:  [1.32669378]\n",
            "Accuracy:  75.844\n",
            "\n",
            "\n",
            "Loss:  [1.34174733]\n",
            "Accuracy:  76.628\n",
            "\n",
            "\n",
            "Loss:  [0.20518376]\n",
            "Accuracy:  76.94\n",
            "\n",
            "\n",
            "Loss:  [0.18514341]\n",
            "Accuracy:  77.316\n",
            "\n",
            "\n",
            "Loss:  [1.59075071]\n",
            "Accuracy:  77.584\n",
            "\n",
            "\n",
            "Loss:  [0.28117521]\n",
            "Accuracy:  77.66\n",
            "\n",
            "\n",
            "Loss:  [1.76142933]\n",
            "Accuracy:  77.708\n",
            "\n",
            "\n",
            "Loss:  [0.20401351]\n",
            "Accuracy:  78.508\n",
            "\n",
            "\n",
            "Loss:  [0.2016698]\n",
            "Accuracy:  78.04\n",
            "\n",
            "\n",
            "Loss:  [1.48584597]\n",
            "Accuracy:  78.304\n",
            "\n",
            "\n",
            "Loss:  [0.22780478]\n",
            "Accuracy:  78.28\n",
            "\n",
            "\n",
            "Loss:  [1.21946796]\n",
            "Accuracy:  77.98\n",
            "\n",
            "\n",
            "Loss:  [0.1709634]\n",
            "Accuracy:  78.004\n",
            "\n",
            "\n",
            "Loss:  [0.18551723]\n",
            "Accuracy:  78.456\n",
            "\n",
            "\n",
            "Loss:  [0.21629308]\n",
            "Accuracy:  78.452\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  76.368\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69345986]\n",
            "Accuracy:  2.34\n",
            "\n",
            "\n",
            "Loss:  [0.71145967]\n",
            "Accuracy:  54.004\n",
            "\n",
            "\n",
            "Loss:  [0.45694211]\n",
            "Accuracy:  69.476\n",
            "\n",
            "\n",
            "Loss:  [0.33606554]\n",
            "Accuracy:  76.024\n",
            "\n",
            "\n",
            "Loss:  [0.26529555]\n",
            "Accuracy:  76.524\n",
            "\n",
            "\n",
            "Loss:  [0.26942889]\n",
            "Accuracy:  76.692\n",
            "\n",
            "\n",
            "Loss:  [0.26305745]\n",
            "Accuracy:  76.82\n",
            "\n",
            "\n",
            "Loss:  [0.32430587]\n",
            "Accuracy:  77.34\n",
            "\n",
            "\n",
            "Loss:  [0.33285416]\n",
            "Accuracy:  77.472\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  76.292\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.0005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69326877]\n",
            "Accuracy:  0.052\n",
            "\n",
            "\n",
            "Loss:  [0.69328758]\n",
            "Accuracy:  16.776\n",
            "\n",
            "\n",
            "Loss:  [0.69427665]\n",
            "Accuracy:  55.212\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  57.868\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.0001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.6931766]\n",
            "Accuracy:  1.224\n",
            "\n",
            "\n",
            "Loss:  [0.69317192]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "Loss:  [0.6931704]\n",
            "Accuracy:  0.016\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  50.028\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.1 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.64919735]\n",
            "Accuracy:  47.648\n",
            "\n",
            "\n",
            "Loss:  [0.34881597]\n",
            "Accuracy:  50.516\n",
            "\n",
            "\n",
            "Loss:  [3.1430126]\n",
            "Accuracy:  53.952\n",
            "\n",
            "\n",
            "Loss:  [3.84150952]\n",
            "Accuracy:  51.716\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  60.14\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.05 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.64470458]\n",
            "Accuracy:  57.452\n",
            "\n",
            "\n",
            "Loss:  [2.02974913]\n",
            "Accuracy:  63.032\n",
            "\n",
            "\n",
            "Loss:  [0.78433245]\n",
            "Accuracy:  64.0\n",
            "\n",
            "\n",
            "Loss:  [0.24966255]\n",
            "Accuracy:  65.888\n",
            "\n",
            "\n",
            "Loss:  [1.36055144]\n",
            "Accuracy:  67.52\n",
            "\n",
            "\n",
            "Loss:  [2.01804921]\n",
            "Accuracy:  68.696\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  71.232\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.01 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.39985761]\n",
            "Accuracy:  61.288\n",
            "\n",
            "\n",
            "Loss:  [1.51442275]\n",
            "Accuracy:  72.888\n",
            "\n",
            "\n",
            "Loss:  [1.91234916]\n",
            "Accuracy:  74.248\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  76.084\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.40785597]\n",
            "Accuracy:  53.32\n",
            "\n",
            "\n",
            "Loss:  [1.34916363]\n",
            "Accuracy:  73.692\n",
            "\n",
            "\n",
            "Loss:  [0.23438777]\n",
            "Accuracy:  76.288\n",
            "\n",
            "\n",
            "Loss:  [2.08797717]\n",
            "Accuracy:  77.236\n",
            "\n",
            "\n",
            "Loss:  [1.77308015]\n",
            "Accuracy:  77.02\n",
            "\n",
            "\n",
            "Loss:  [1.82687938]\n",
            "Accuracy:  78.012\n",
            "\n",
            "\n",
            "Loss:  [0.49804472]\n",
            "Accuracy:  77.668\n",
            "\n",
            "\n",
            "Loss:  [0.47678551]\n",
            "Accuracy:  78.828\n",
            "\n",
            "\n",
            "Loss:  [0.32911522]\n",
            "Accuracy:  79.028\n",
            "\n",
            "\n",
            "Loss:  [0.36056225]\n",
            "Accuracy:  79.144\n",
            "\n",
            "\n",
            "Loss:  [0.37640373]\n",
            "Accuracy:  79.22\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  78.996\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.6934818]\n",
            "Accuracy:  6.924\n",
            "\n",
            "\n",
            "Loss:  [0.72062112]\n",
            "Accuracy:  56.844\n",
            "\n",
            "\n",
            "Loss:  [0.30188204]\n",
            "Accuracy:  72.232\n",
            "\n",
            "\n",
            "Loss:  [1.7553063]\n",
            "Accuracy:  76.292\n",
            "\n",
            "\n",
            "Loss:  [0.25920194]\n",
            "Accuracy:  76.756\n",
            "\n",
            "\n",
            "Loss:  [1.72941833]\n",
            "Accuracy:  77.08\n",
            "\n",
            "\n",
            "Loss:  [0.75528485]\n",
            "Accuracy:  77.02\n",
            "\n",
            "\n",
            "Loss:  [1.68847809]\n",
            "Accuracy:  77.324\n",
            "\n",
            "\n",
            "Loss:  [0.34052586]\n",
            "Accuracy:  77.38\n",
            "\n",
            "\n",
            "Loss:  [0.49085723]\n",
            "Accuracy:  77.744\n",
            "\n",
            "\n",
            "Loss:  [0.26908242]\n",
            "Accuracy:  78.116\n",
            "\n",
            "\n",
            "Loss:  [0.27757554]\n",
            "Accuracy:  78.24\n",
            "\n",
            "\n",
            "Loss:  [0.40080817]\n",
            "Accuracy:  78.36\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  78.34\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.0005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69326822]\n",
            "Accuracy:  0.084\n",
            "\n",
            "\n",
            "Loss:  [0.69332487]\n",
            "Accuracy:  28.928\n",
            "\n",
            "\n",
            "Loss:  [0.69534436]\n",
            "Accuracy:  56.848\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  57.952\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.0001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69318178]\n",
            "Accuracy:  1.364\n",
            "\n",
            "\n",
            "Loss:  [0.69317757]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "Loss:  [0.69317728]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  50.0\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.1 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.44133196]\n",
            "Accuracy:  40.98\n",
            "\n",
            "\n",
            "Loss:  [3.86299646]\n",
            "Accuracy:  38.988\n",
            "\n",
            "\n",
            "Loss:  [0.78487863]\n",
            "Accuracy:  43.432\n",
            "\n",
            "\n",
            "Loss:  [0.9255898]\n",
            "Accuracy:  41.496\n",
            "\n",
            "\n",
            "Loss:  [0.89961477]\n",
            "Accuracy:  42.7\n",
            "\n",
            "\n",
            "Loss:  [0.33781338]\n",
            "Accuracy:  44.108\n",
            "\n",
            "\n",
            "Loss:  [1.9574337]\n",
            "Accuracy:  43.912\n",
            "\n",
            "\n",
            "Loss:  [0.39261947]\n",
            "Accuracy:  40.824\n",
            "\n",
            "\n",
            "Loss:  [0.53775657]\n",
            "Accuracy:  45.408\n",
            "\n",
            "\n",
            "Loss:  [0.45927895]\n",
            "Accuracy:  50.856\n",
            "\n",
            "\n",
            "Loss:  [0.82150684]\n",
            "Accuracy:  50.364\n",
            "\n",
            "\n",
            "Loss:  [0.05339428]\n",
            "Accuracy:  50.596\n",
            "\n",
            "\n",
            "Loss:  [1.59319751]\n",
            "Accuracy:  50.996\n",
            "\n",
            "\n",
            "Loss:  [1.41182215]\n",
            "Accuracy:  50.728\n",
            "\n",
            "\n",
            "Loss:  [0.02041898]\n",
            "Accuracy:  50.8\n",
            "\n",
            "\n",
            "Loss:  [1.64598425]\n",
            "Accuracy:  51.376\n",
            "\n",
            "\n",
            "Loss:  [0.08043669]\n",
            "Accuracy:  50.4\n",
            "\n",
            "\n",
            "Loss:  [0.2100551]\n",
            "Accuracy:  48.58\n",
            "\n",
            "\n",
            "Loss:  [0.16017774]\n",
            "Accuracy:  49.996\n",
            "\n",
            "\n",
            "Loss:  [0.11889308]\n",
            "Accuracy:  50.876\n",
            "\n",
            "\n",
            "Loss:  [0.06668997]\n",
            "Accuracy:  49.6\n",
            "\n",
            "\n",
            "Loss:  [0.3868303]\n",
            "Accuracy:  49.036\n",
            "\n",
            "\n",
            "Loss:  [0.41115918]\n",
            "Accuracy:  50.208\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  58.7\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.05 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [1.6775665]\n",
            "Accuracy:  56.68\n",
            "\n",
            "\n",
            "Loss:  [1.37953698]\n",
            "Accuracy:  56.656\n",
            "\n",
            "\n",
            "Loss:  [1.51416299]\n",
            "Accuracy:  53.364\n",
            "\n",
            "\n",
            "Loss:  [1.91485961]\n",
            "Accuracy:  52.68\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  60.96\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.01 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [1.52827913]\n",
            "Accuracy:  60.712\n",
            "\n",
            "\n",
            "Loss:  [1.41128698]\n",
            "Accuracy:  71.132\n",
            "\n",
            "\n",
            "Loss:  [0.84520398]\n",
            "Accuracy:  73.26\n",
            "\n",
            "\n",
            "Loss:  [1.56740223]\n",
            "Accuracy:  74.54\n",
            "\n",
            "\n",
            "Loss:  [0.86737437]\n",
            "Accuracy:  75.26\n",
            "\n",
            "\n",
            "Loss:  [2.35581692]\n",
            "Accuracy:  75.256\n",
            "\n",
            "\n",
            "Loss:  [0.82535568]\n",
            "Accuracy:  76.696\n",
            "\n",
            "\n",
            "Loss:  [1.25499626]\n",
            "Accuracy:  76.416\n",
            "\n",
            "\n",
            "Loss:  [0.3503561]\n",
            "Accuracy:  76.78\n",
            "\n",
            "\n",
            "Loss:  [0.52132348]\n",
            "Accuracy:  77.468\n",
            "\n",
            "\n",
            "Loss:  [0.73053926]\n",
            "Accuracy:  76.42\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  77.416\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.276104]\n",
            "Accuracy:  55.12\n",
            "\n",
            "\n",
            "Loss:  [0.26749061]\n",
            "Accuracy:  73.472\n",
            "\n",
            "\n",
            "Loss:  [1.45851455]\n",
            "Accuracy:  74.436\n",
            "\n",
            "\n",
            "Loss:  [0.30694948]\n",
            "Accuracy:  76.344\n",
            "\n",
            "\n",
            "Loss:  [0.29485418]\n",
            "Accuracy:  77.128\n",
            "\n",
            "\n",
            "Loss:  [0.30076838]\n",
            "Accuracy:  77.54\n",
            "\n",
            "\n",
            "Loss:  [1.5592604]\n",
            "Accuracy:  77.884\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  78.648\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.6938078]\n",
            "Accuracy:  16.888\n",
            "\n",
            "\n",
            "Loss:  [0.76876711]\n",
            "Accuracy:  58.936\n",
            "\n",
            "\n",
            "Loss:  [0.42320243]\n",
            "Accuracy:  74.636\n",
            "\n",
            "\n",
            "Loss:  [0.3123563]\n",
            "Accuracy:  77.324\n",
            "\n",
            "\n",
            "Loss:  [0.3280022]\n",
            "Accuracy:  77.36\n",
            "\n",
            "\n",
            "Loss:  [0.29568133]\n",
            "Accuracy:  77.692\n",
            "\n",
            "\n",
            "Loss:  [0.3608833]\n",
            "Accuracy:  77.568\n",
            "\n",
            "\n",
            "Loss:  [1.67468797]\n",
            "Accuracy:  77.62\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  77.22\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.0005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69331019]\n",
            "Accuracy:  0.12\n",
            "\n",
            "\n",
            "Loss:  [0.69359651]\n",
            "Accuracy:  35.504\n",
            "\n",
            "\n",
            "Loss:  [0.69733519]\n",
            "Accuracy:  57.1\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  58.32\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.0001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69317233]\n",
            "Accuracy:  2.04\n",
            "\n",
            "\n",
            "Loss:  [0.6931644]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "Loss:  [0.69315868]\n",
            "Accuracy:  0.052\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  50.072\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  50 \n",
            "alpha =  0.1 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [8.76918955]\n",
            "Accuracy:  19.852\n",
            "\n",
            "\n",
            "Loss:  [2.10079823]\n",
            "Accuracy:  19.66\n",
            "\n",
            "\n",
            "Loss:  [8.85117253]\n",
            "Accuracy:  23.272\n",
            "\n",
            "\n",
            "Loss:  [0.12913529]\n",
            "Accuracy:  19.376\n",
            "\n",
            "\n",
            "Loss:  [7.18764684]\n",
            "Accuracy:  23.42\n",
            "\n",
            "\n",
            "Loss:  [1.54810437]\n",
            "Accuracy:  22.512\n",
            "\n",
            "\n",
            "Loss:  [2.29238282]\n",
            "Accuracy:  26.328\n",
            "\n",
            "\n",
            "Loss:  [3.75483736]\n",
            "Accuracy:  32.124\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  53.616\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  50 \n",
            "alpha =  0.05 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.63434358]\n",
            "Accuracy:  48.896\n",
            "\n",
            "\n",
            "Loss:  [1.59093072]\n",
            "Accuracy:  27.228\n",
            "\n",
            "\n",
            "Loss:  [4.4782543]\n",
            "Accuracy:  29.792\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  51.132\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  50 \n",
            "alpha =  0.01 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.28168295]\n",
            "Accuracy:  60.364\n",
            "\n",
            "\n",
            "Loss:  [1.23807137]\n",
            "Accuracy:  70.536\n",
            "\n",
            "\n",
            "Loss:  [1.29548813]\n",
            "Accuracy:  71.348\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  70.3\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  50 \n",
            "alpha =  0.005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.48131181]\n",
            "Accuracy:  54.824\n",
            "\n",
            "\n",
            "Loss:  [0.44033036]\n",
            "Accuracy:  72.536\n",
            "\n",
            "\n",
            "Loss:  [2.09111767]\n",
            "Accuracy:  73.352\n",
            "\n",
            "\n",
            "Loss:  [0.53558037]\n",
            "Accuracy:  74.94\n",
            "\n",
            "\n",
            "Loss:  [0.52295106]\n",
            "Accuracy:  75.452\n",
            "\n",
            "\n",
            "Loss:  [1.611767]\n",
            "Accuracy:  75.028\n",
            "\n",
            "\n",
            "Loss:  [1.91353829]\n",
            "Accuracy:  74.716\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  71.26\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  50 \n",
            "alpha =  0.001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69368109]\n",
            "Accuracy:  16.676\n",
            "\n",
            "\n",
            "Loss:  [0.77332542]\n",
            "Accuracy:  59.0\n",
            "\n",
            "\n",
            "Loss:  [0.29967596]\n",
            "Accuracy:  75.076\n",
            "\n",
            "\n",
            "Loss:  [0.23307157]\n",
            "Accuracy:  77.264\n",
            "\n",
            "\n",
            "Loss:  [0.22921197]\n",
            "Accuracy:  77.516\n",
            "\n",
            "\n",
            "Loss:  [0.28235184]\n",
            "Accuracy:  77.692\n",
            "\n",
            "\n",
            "Loss:  [0.23656832]\n",
            "Accuracy:  78.136\n",
            "\n",
            "\n",
            "Loss:  [0.23048922]\n",
            "Accuracy:  78.316\n",
            "\n",
            "\n",
            "Loss:  [0.2590562]\n",
            "Accuracy:  78.544\n",
            "\n",
            "\n",
            "Loss:  [0.50019168]\n",
            "Accuracy:  78.552\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  77.008\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  50 \n",
            "alpha =  0.0005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69327115]\n",
            "Accuracy:  2.232\n",
            "\n",
            "\n",
            "Loss:  [0.69351185]\n",
            "Accuracy:  46.552\n",
            "\n",
            "\n",
            "Loss:  [0.69897086]\n",
            "Accuracy:  57.768\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  58.636\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  50 \n",
            "alpha =  0.0001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69318077]\n",
            "Accuracy:  14.06\n",
            "\n",
            "\n",
            "Loss:  [0.69318463]\n",
            "Accuracy:  4.276\n",
            "\n",
            "\n",
            "Loss:  [0.69319473]\n",
            "Accuracy:  19.244\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  55.98\n",
            "\n",
            " _________________________________________________________ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTYsE7Dxd8n7",
        "colab_type": "text"
      },
      "source": [
        "Apart from the above, we can also do a bit more preprocessing. This will involve removing the less frequent words (say taking all words whose frequency in the entire text is more than 50). I've also shown that below. It sometimes led to about 1% increase in accuracy (But taking words whose freq>100 led to the similar or less accuracy). But thats not very much considering the fact that neural nets can get trained in different ways every time. Also choice of this number (50) is very biased (based on the data). So if we are getting the similar accuracy without this preprocessing, then we shouldn't worry much about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L3XtudNZOKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "\n",
        "combined_str=(\" \".join(pos_train))+(\" \".join(neg_train)) #Combining all strings into one.                    \n",
        "\n",
        "words=word_tokenize(combined_str)\n",
        "cfdist=FreqDist(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ACNavRXZPS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens=set()\n",
        "for w in words:\n",
        "    if(cfdist[w]>50): #Take all words whose freq > 50\n",
        "        tokens.add(w)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9evgraxBfT3I",
        "colab": {}
      },
      "source": [
        "def createInputs(text):\n",
        "  #The same as before just with a check for word in tokens or not\n",
        "  inputs=[]\n",
        "  for w in text.split(' '):\n",
        "    if(w in embeddings_dict and w in tokens):\n",
        "        v=embeddings_dict[w]\n",
        "        v.shape=[300,1]\n",
        "        inputs.append(v)\n",
        "  return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBRswrp2ZrRq",
        "colab_type": "code",
        "outputId": "caf65712-f8ab-4853-e0cb-6d31323987c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(h=20,epochs=40,alpha=0.005)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training has begun...\n",
            "\n",
            "Loss:  [1.56105685]\n",
            "Accuracy:  49.568\n",
            "\n",
            "\n",
            "Loss:  [0.19262998]\n",
            "Accuracy:  73.66\n",
            "\n",
            "\n",
            "Loss:  [0.22237042]\n",
            "Accuracy:  75.44\n",
            "\n",
            "\n",
            "Loss:  [0.32196493]\n",
            "Accuracy:  75.576\n",
            "\n",
            "\n",
            "Loss:  [0.18680813]\n",
            "Accuracy:  76.888\n",
            "\n",
            "\n",
            "Loss:  [1.71801747]\n",
            "Accuracy:  76.668\n",
            "\n",
            "\n",
            "Loss:  [0.27449335]\n",
            "Accuracy:  76.956\n",
            "\n",
            "\n",
            "Loss:  [0.62961238]\n",
            "Accuracy:  77.332\n",
            "\n",
            "\n",
            "Loss:  [0.86284827]\n",
            "Accuracy:  78.064\n",
            "\n",
            "\n",
            "Loss:  [0.86123064]\n",
            "Accuracy:  78.196\n",
            "\n",
            "\n",
            "Loss:  [0.76497505]\n",
            "Accuracy:  79.324\n",
            "\n",
            "\n",
            "Loss:  [0.54380779]\n",
            "Accuracy:  78.908\n",
            "\n",
            "\n",
            "Loss:  [0.3205685]\n",
            "Accuracy:  78.676\n",
            "\n",
            "\n",
            "Loss:  [0.12292897]\n",
            "Accuracy:  78.62\n",
            "\n",
            "\n",
            "Loss:  [0.31417054]\n",
            "Accuracy:  78.84\n",
            "\n",
            "\n",
            "Loss:  [0.1760929]\n",
            "Accuracy:  79.556\n",
            "\n",
            "\n",
            "Loss:  [0.05627441]\n",
            "Accuracy:  79.604\n",
            "\n",
            "\n",
            "Loss:  [0.09410214]\n",
            "Accuracy:  78.84\n",
            "\n",
            "\n",
            "Loss:  [0.36293672]\n",
            "Accuracy:  79.836\n",
            "\n",
            "\n",
            "Loss:  [0.04828335]\n",
            "Accuracy:  79.884\n",
            "\n",
            "\n",
            "Loss:  [0.48108264]\n",
            "Accuracy:  79.964\n",
            "\n",
            "\n",
            "Loss:  [0.32584473]\n",
            "Accuracy:  80.1\n",
            "\n",
            "\n",
            "Loss:  [1.26503827]\n",
            "Accuracy:  80.324\n",
            "\n",
            "\n",
            "Loss:  [0.05949451]\n",
            "Accuracy:  80.5\n",
            "\n",
            "\n",
            "Loss:  [0.67925149]\n",
            "Accuracy:  80.736\n",
            "\n",
            "\n",
            "Loss:  [0.12836423]\n",
            "Accuracy:  80.172\n",
            "\n",
            "\n",
            "Loss:  [0.98284004]\n",
            "Accuracy:  79.496\n",
            "\n",
            "\n",
            "Loss:  [0.28215106]\n",
            "Accuracy:  80.316\n",
            "\n",
            "\n",
            "Loss:  [1.0780695]\n",
            "Accuracy:  80.896\n",
            "\n",
            "\n",
            "Loss:  [1.15614775]\n",
            "Accuracy:  80.896\n",
            "\n",
            "\n",
            "Loss:  [0.80338273]\n",
            "Accuracy:  80.668\n",
            "\n",
            "\n",
            "Loss:  [1.0052374]\n",
            "Accuracy:  80.52\n",
            "\n",
            "\n",
            "Loss:  [1.59980015]\n",
            "Accuracy:  80.712\n",
            "\n",
            "\n",
            "Loss:  [0.62315613]\n",
            "Accuracy:  81.18\n",
            "\n",
            "\n",
            "Loss:  [0.67589958]\n",
            "Accuracy:  81.568\n",
            "\n",
            "\n",
            "Loss:  [0.33028853]\n",
            "Accuracy:  81.008\n",
            "\n",
            "\n",
            "Loss:  [0.3224333]\n",
            "Accuracy:  80.78\n",
            "\n",
            "\n",
            "Loss:  [0.40876514]\n",
            "Accuracy:  80.748\n",
            "\n",
            "\n",
            "Loss:  [0.08516888]\n",
            "Accuracy:  81.372\n",
            "\n",
            "\n",
            "Loss:  [0.3013972]\n",
            "Accuracy:  81.012\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  80.532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCdH7mKwfdlI",
        "colab_type": "text"
      },
      "source": [
        "So we get a training accuracy of 81% and a test accuracy of 80.5%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOZLd8cbq44",
        "colab_type": "text"
      },
      "source": [
        "There is one more way that may seem not to work but actually worked better than this one. It is taking the individual sentences in a review as a combined vector of words - the average of the GloVe vectors for each word in that sentence. The advantage may be that it would be very fast and be able to learn better. But the disadvantage is that the order of words is not preserved. This is the thing that RNN specializes in. The method is shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEic50gyclJ4",
        "colab": {}
      },
      "source": [
        "def createInputs(text):\n",
        "  #Returns the average of GLOVE vectors for the sentences in the review \n",
        "  inputs=[]\n",
        "  for sentence in re.split('[.?!]+',text):\n",
        "      temp=np.zeros((300,1))\n",
        "      len=0\n",
        "      for w in sentence.split(' '):\n",
        "          if(w in embeddings_dict):\n",
        "              v=embeddings_dict[w]\n",
        "              v.shape=[300,1]\n",
        "              temp+=v\n",
        "              len+=1\n",
        "      if(len>0):\n",
        "          inputs.append(temp/len)\n",
        "  return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N8CO3lJP-ld",
        "colab_type": "code",
        "outputId": "09071102-c96d-41f7-a8fd-f4b42bae3dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Grid search for best hyperparameters\n",
        "for h in [5,10,20]:\n",
        "    for alpha in [0.1,0.05,0.01,0.005,0.001,0.0001]:\n",
        "        print(\"h = \",h,\"\\nalpha = \",alpha,\"\\n\")\n",
        "        train(h=h,epochs=40,alpha=alpha)\n",
        "        print(\"\\n _________________________________________________________ \\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h =  5 \n",
            "alpha =  0.1 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.20087626]\n",
            "Accuracy:  49.728\n",
            "\n",
            "\n",
            "Loss:  [0.54937452]\n",
            "Accuracy:  64.244\n",
            "\n",
            "\n",
            "Loss:  [0.22720234]\n",
            "Accuracy:  64.432\n",
            "\n",
            "\n",
            "Loss:  [0.1775706]\n",
            "Accuracy:  65.076\n",
            "\n",
            "\n",
            "Loss:  [0.15365852]\n",
            "Accuracy:  67.836\n",
            "\n",
            "\n",
            "Loss:  [0.09890166]\n",
            "Accuracy:  66.272\n",
            "\n",
            "\n",
            "Loss:  [0.04764218]\n",
            "Accuracy:  67.148\n",
            "\n",
            "\n",
            "Loss:  [0.61974319]\n",
            "Accuracy:  64.176\n",
            "\n",
            "\n",
            "Loss:  [0.12867393]\n",
            "Accuracy:  69.376\n",
            "\n",
            "\n",
            "Loss:  [0.04756756]\n",
            "Accuracy:  66.612\n",
            "\n",
            "\n",
            "Loss:  [0.15844039]\n",
            "Accuracy:  69.872\n",
            "\n",
            "\n",
            "Loss:  [0.08370072]\n",
            "Accuracy:  71.56\n",
            "\n",
            "\n",
            "Loss:  [0.23445456]\n",
            "Accuracy:  72.332\n",
            "\n",
            "\n",
            "Loss:  [0.09952273]\n",
            "Accuracy:  72.436\n",
            "\n",
            "\n",
            "Loss:  [0.46848572]\n",
            "Accuracy:  73.06\n",
            "\n",
            "\n",
            "Loss:  [0.30878677]\n",
            "Accuracy:  73.408\n",
            "\n",
            "\n",
            "Loss:  [0.6402406]\n",
            "Accuracy:  72.156\n",
            "\n",
            "\n",
            "Loss:  [0.05566866]\n",
            "Accuracy:  73.472\n",
            "\n",
            "\n",
            "Loss:  [0.29056602]\n",
            "Accuracy:  73.02\n",
            "\n",
            "\n",
            "Loss:  [0.23854305]\n",
            "Accuracy:  73.336\n",
            "\n",
            "\n",
            "Loss:  [0.36903676]\n",
            "Accuracy:  68.84\n",
            "\n",
            "\n",
            "Loss:  [0.532831]\n",
            "Accuracy:  72.296\n",
            "\n",
            "\n",
            "Loss:  [0.1075567]\n",
            "Accuracy:  74.728\n",
            "\n",
            "\n",
            "Loss:  [0.07695135]\n",
            "Accuracy:  71.612\n",
            "\n",
            "\n",
            "Loss:  [0.51587096]\n",
            "Accuracy:  72.788\n",
            "\n",
            "\n",
            "Loss:  [0.44308085]\n",
            "Accuracy:  72.576\n",
            "\n",
            "\n",
            "Loss:  [0.14436246]\n",
            "Accuracy:  74.724\n",
            "\n",
            "\n",
            "Loss:  [0.11168895]\n",
            "Accuracy:  73.084\n",
            "\n",
            "\n",
            "Loss:  [0.05715484]\n",
            "Accuracy:  74.52\n",
            "\n",
            "\n",
            "Loss:  [0.08677474]\n",
            "Accuracy:  74.976\n",
            "\n",
            "\n",
            "Loss:  [0.07383829]\n",
            "Accuracy:  75.692\n",
            "\n",
            "\n",
            "Loss:  [0.58217413]\n",
            "Accuracy:  75.648\n",
            "\n",
            "\n",
            "Loss:  [0.07837241]\n",
            "Accuracy:  75.008\n",
            "\n",
            "\n",
            "Loss:  [0.1686612]\n",
            "Accuracy:  75.148\n",
            "\n",
            "\n",
            "Loss:  [0.13724272]\n",
            "Accuracy:  74.684\n",
            "\n",
            "\n",
            "Loss:  [0.09965408]\n",
            "Accuracy:  75.812\n",
            "\n",
            "\n",
            "Loss:  [0.14764571]\n",
            "Accuracy:  76.028\n",
            "\n",
            "\n",
            "Loss:  [0.04344487]\n",
            "Accuracy:  75.136\n",
            "\n",
            "\n",
            "Loss:  [0.15690612]\n",
            "Accuracy:  74.996\n",
            "\n",
            "\n",
            "Loss:  [0.04840322]\n",
            "Accuracy:  76.56\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  75.592\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.05 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.29679407]\n",
            "Accuracy:  58.452\n",
            "\n",
            "\n",
            "Loss:  [0.16002795]\n",
            "Accuracy:  73.348\n",
            "\n",
            "\n",
            "Loss:  [0.0593167]\n",
            "Accuracy:  74.292\n",
            "\n",
            "\n",
            "Loss:  [0.1407491]\n",
            "Accuracy:  75.684\n",
            "\n",
            "\n",
            "Loss:  [0.244508]\n",
            "Accuracy:  76.164\n",
            "\n",
            "\n",
            "Loss:  [0.09546578]\n",
            "Accuracy:  76.648\n",
            "\n",
            "\n",
            "Loss:  [0.45334312]\n",
            "Accuracy:  76.136\n",
            "\n",
            "\n",
            "Loss:  [0.2497738]\n",
            "Accuracy:  77.108\n",
            "\n",
            "\n",
            "Loss:  [0.35181825]\n",
            "Accuracy:  77.928\n",
            "\n",
            "\n",
            "Loss:  [0.09887575]\n",
            "Accuracy:  76.824\n",
            "\n",
            "\n",
            "Loss:  [0.14152414]\n",
            "Accuracy:  77.36\n",
            "\n",
            "\n",
            "Loss:  [0.09181582]\n",
            "Accuracy:  78.092\n",
            "\n",
            "\n",
            "Loss:  [0.60554074]\n",
            "Accuracy:  78.1\n",
            "\n",
            "\n",
            "Loss:  [0.22179244]\n",
            "Accuracy:  78.208\n",
            "\n",
            "\n",
            "Loss:  [0.312346]\n",
            "Accuracy:  77.716\n",
            "\n",
            "\n",
            "Loss:  [0.16258382]\n",
            "Accuracy:  78.064\n",
            "\n",
            "\n",
            "Loss:  [0.1529202]\n",
            "Accuracy:  78.232\n",
            "\n",
            "\n",
            "Loss:  [0.09557144]\n",
            "Accuracy:  78.42\n",
            "\n",
            "\n",
            "Loss:  [0.02948285]\n",
            "Accuracy:  78.728\n",
            "\n",
            "\n",
            "Loss:  [0.10795516]\n",
            "Accuracy:  78.664\n",
            "\n",
            "\n",
            "Loss:  [0.13681988]\n",
            "Accuracy:  79.176\n",
            "\n",
            "\n",
            "Loss:  [0.08620609]\n",
            "Accuracy:  78.692\n",
            "\n",
            "\n",
            "Loss:  [0.27848233]\n",
            "Accuracy:  78.996\n",
            "\n",
            "\n",
            "Loss:  [0.25848309]\n",
            "Accuracy:  78.868\n",
            "\n",
            "\n",
            "Loss:  [0.19529229]\n",
            "Accuracy:  78.996\n",
            "\n",
            "\n",
            "Loss:  [0.15789688]\n",
            "Accuracy:  78.38\n",
            "\n",
            "\n",
            "Loss:  [0.23574128]\n",
            "Accuracy:  79.132\n",
            "\n",
            "\n",
            "Loss:  [0.11182415]\n",
            "Accuracy:  79.076\n",
            "\n",
            "\n",
            "Loss:  [0.11511801]\n",
            "Accuracy:  79.072\n",
            "\n",
            "\n",
            "Loss:  [0.26127374]\n",
            "Accuracy:  79.052\n",
            "\n",
            "\n",
            "Loss:  [0.2407761]\n",
            "Accuracy:  78.98\n",
            "\n",
            "\n",
            "Loss:  [0.06228939]\n",
            "Accuracy:  78.036\n",
            "\n",
            "\n",
            "Loss:  [0.23262396]\n",
            "Accuracy:  78.788\n",
            "\n",
            "\n",
            "Loss:  [0.16023778]\n",
            "Accuracy:  79.024\n",
            "\n",
            "\n",
            "Loss:  [0.21612696]\n",
            "Accuracy:  79.324\n",
            "\n",
            "\n",
            "Loss:  [0.65240291]\n",
            "Accuracy:  79.468\n",
            "\n",
            "\n",
            "Loss:  [0.2271343]\n",
            "Accuracy:  79.156\n",
            "\n",
            "\n",
            "Loss:  [0.17992288]\n",
            "Accuracy:  79.728\n",
            "\n",
            "\n",
            "Loss:  [0.23699689]\n",
            "Accuracy:  79.52\n",
            "\n",
            "\n",
            "Loss:  [0.32986736]\n",
            "Accuracy:  79.436\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  79.688\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.01 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.16725415]\n",
            "Accuracy:  50.404\n",
            "\n",
            "\n",
            "Loss:  [0.13325548]\n",
            "Accuracy:  77.076\n",
            "\n",
            "\n",
            "Loss:  [0.10518564]\n",
            "Accuracy:  78.204\n",
            "\n",
            "\n",
            "Loss:  [0.1071541]\n",
            "Accuracy:  79.08\n",
            "\n",
            "\n",
            "Loss:  [0.12701944]\n",
            "Accuracy:  79.68\n",
            "\n",
            "\n",
            "Loss:  [0.11357744]\n",
            "Accuracy:  79.92\n",
            "\n",
            "\n",
            "Loss:  [0.16597032]\n",
            "Accuracy:  80.244\n",
            "\n",
            "\n",
            "Loss:  [0.15111046]\n",
            "Accuracy:  80.608\n",
            "\n",
            "\n",
            "Loss:  [0.18738171]\n",
            "Accuracy:  80.908\n",
            "\n",
            "\n",
            "Loss:  [0.24797611]\n",
            "Accuracy:  80.94\n",
            "\n",
            "\n",
            "Loss:  [0.20470336]\n",
            "Accuracy:  81.1\n",
            "\n",
            "\n",
            "Loss:  [0.1494349]\n",
            "Accuracy:  81.196\n",
            "\n",
            "\n",
            "Loss:  [0.12059616]\n",
            "Accuracy:  81.392\n",
            "\n",
            "\n",
            "Loss:  [0.48461788]\n",
            "Accuracy:  81.34\n",
            "\n",
            "\n",
            "Loss:  [0.09736506]\n",
            "Accuracy:  81.052\n",
            "\n",
            "\n",
            "Loss:  [0.19283398]\n",
            "Accuracy:  81.392\n",
            "\n",
            "\n",
            "Loss:  [0.12318829]\n",
            "Accuracy:  81.404\n",
            "\n",
            "\n",
            "Loss:  [0.11864564]\n",
            "Accuracy:  81.432\n",
            "\n",
            "\n",
            "Loss:  [0.09190165]\n",
            "Accuracy:  81.632\n",
            "\n",
            "\n",
            "Loss:  [0.11122695]\n",
            "Accuracy:  81.616\n",
            "\n",
            "\n",
            "Loss:  [0.10380421]\n",
            "Accuracy:  81.656\n",
            "\n",
            "\n",
            "Loss:  [0.13461055]\n",
            "Accuracy:  81.836\n",
            "\n",
            "\n",
            "Loss:  [0.07836913]\n",
            "Accuracy:  81.856\n",
            "\n",
            "\n",
            "Loss:  [0.11590423]\n",
            "Accuracy:  81.932\n",
            "\n",
            "\n",
            "Loss:  [0.13145551]\n",
            "Accuracy:  82.08\n",
            "\n",
            "\n",
            "Loss:  [0.11609339]\n",
            "Accuracy:  82.02\n",
            "\n",
            "\n",
            "Loss:  [0.09800722]\n",
            "Accuracy:  81.788\n",
            "\n",
            "\n",
            "Loss:  [0.1182015]\n",
            "Accuracy:  81.908\n",
            "\n",
            "\n",
            "Loss:  [0.08645289]\n",
            "Accuracy:  81.844\n",
            "\n",
            "\n",
            "Loss:  [0.09510049]\n",
            "Accuracy:  82.036\n",
            "\n",
            "\n",
            "Loss:  [0.09264479]\n",
            "Accuracy:  82.172\n",
            "\n",
            "\n",
            "Loss:  [0.12755947]\n",
            "Accuracy:  81.988\n",
            "\n",
            "\n",
            "Loss:  [0.09487125]\n",
            "Accuracy:  82.08\n",
            "\n",
            "\n",
            "Loss:  [0.14443787]\n",
            "Accuracy:  82.628\n",
            "\n",
            "\n",
            "Loss:  [0.10826368]\n",
            "Accuracy:  82.036\n",
            "\n",
            "\n",
            "Loss:  [0.0918294]\n",
            "Accuracy:  82.392\n",
            "\n",
            "\n",
            "Loss:  [0.12555461]\n",
            "Accuracy:  82.452\n",
            "\n",
            "\n",
            "Loss:  [0.11800251]\n",
            "Accuracy:  82.352\n",
            "\n",
            "\n",
            "Loss:  [0.08806596]\n",
            "Accuracy:  82.512\n",
            "\n",
            "\n",
            "Loss:  [0.09165028]\n",
            "Accuracy:  82.184\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  81.052\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.23404425]\n",
            "Accuracy:  37.268\n",
            "\n",
            "\n",
            "Loss:  [0.15733128]\n",
            "Accuracy:  77.252\n",
            "\n",
            "\n",
            "Loss:  [0.16737596]\n",
            "Accuracy:  78.848\n",
            "\n",
            "\n",
            "Loss:  [0.10733012]\n",
            "Accuracy:  79.504\n",
            "\n",
            "\n",
            "Loss:  [0.10357335]\n",
            "Accuracy:  79.776\n",
            "\n",
            "\n",
            "Loss:  [0.09463413]\n",
            "Accuracy:  80.268\n",
            "\n",
            "\n",
            "Loss:  [0.0911816]\n",
            "Accuracy:  80.888\n",
            "\n",
            "\n",
            "Loss:  [0.08421978]\n",
            "Accuracy:  81.128\n",
            "\n",
            "\n",
            "Loss:  [0.08442639]\n",
            "Accuracy:  81.024\n",
            "\n",
            "\n",
            "Loss:  [0.09584396]\n",
            "Accuracy:  81.208\n",
            "\n",
            "\n",
            "Loss:  [0.09050552]\n",
            "Accuracy:  81.224\n",
            "\n",
            "\n",
            "Loss:  [0.09207401]\n",
            "Accuracy:  81.432\n",
            "\n",
            "\n",
            "Loss:  [0.07422177]\n",
            "Accuracy:  81.564\n",
            "\n",
            "\n",
            "Loss:  [0.08234242]\n",
            "Accuracy:  81.724\n",
            "\n",
            "\n",
            "Loss:  [0.07891946]\n",
            "Accuracy:  82.032\n",
            "\n",
            "\n",
            "Loss:  [0.0702784]\n",
            "Accuracy:  82.224\n",
            "\n",
            "\n",
            "Loss:  [0.07762518]\n",
            "Accuracy:  82.2\n",
            "\n",
            "\n",
            "Loss:  [0.07749223]\n",
            "Accuracy:  82.428\n",
            "\n",
            "\n",
            "Loss:  [0.07415356]\n",
            "Accuracy:  82.652\n",
            "\n",
            "\n",
            "Loss:  [0.07271252]\n",
            "Accuracy:  82.268\n",
            "\n",
            "\n",
            "Loss:  [0.0700176]\n",
            "Accuracy:  82.396\n",
            "\n",
            "\n",
            "Loss:  [0.07348272]\n",
            "Accuracy:  82.46\n",
            "\n",
            "\n",
            "Loss:  [0.06876197]\n",
            "Accuracy:  82.672\n",
            "\n",
            "\n",
            "Loss:  [0.06875611]\n",
            "Accuracy:  82.676\n",
            "\n",
            "\n",
            "Loss:  [0.06623246]\n",
            "Accuracy:  82.76\n",
            "\n",
            "\n",
            "Loss:  [0.06101484]\n",
            "Accuracy:  82.904\n",
            "\n",
            "\n",
            "Loss:  [0.06967235]\n",
            "Accuracy:  82.908\n",
            "\n",
            "\n",
            "Loss:  [0.07828254]\n",
            "Accuracy:  83.1\n",
            "\n",
            "\n",
            "Loss:  [0.07988095]\n",
            "Accuracy:  83.108\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  80.028\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69339077]\n",
            "Accuracy:  0.004\n",
            "\n",
            "\n",
            "Loss:  [0.69327666]\n",
            "Accuracy:  23.304\n",
            "\n",
            "\n",
            "Loss:  [0.67950194]\n",
            "Accuracy:  61.208\n",
            "\n",
            "\n",
            "Loss:  [0.13889428]\n",
            "Accuracy:  74.988\n",
            "\n",
            "\n",
            "Loss:  [0.12596221]\n",
            "Accuracy:  80.024\n",
            "\n",
            "\n",
            "Loss:  [0.13408442]\n",
            "Accuracy:  80.556\n",
            "\n",
            "\n",
            "Loss:  [0.1442684]\n",
            "Accuracy:  80.776\n",
            "\n",
            "\n",
            "Loss:  [0.14320722]\n",
            "Accuracy:  80.82\n",
            "\n",
            "\n",
            "Loss:  [0.13430384]\n",
            "Accuracy:  80.968\n",
            "\n",
            "\n",
            "Loss:  [0.12696184]\n",
            "Accuracy:  81.248\n",
            "\n",
            "\n",
            "Loss:  [0.11822377]\n",
            "Accuracy:  81.428\n",
            "\n",
            "\n",
            "Loss:  [0.11292001]\n",
            "Accuracy:  81.56\n",
            "\n",
            "\n",
            "Loss:  [0.10862945]\n",
            "Accuracy:  81.92\n",
            "\n",
            "\n",
            "Loss:  [0.10922595]\n",
            "Accuracy:  82.16\n",
            "\n",
            "\n",
            "Loss:  [0.10382883]\n",
            "Accuracy:  82.18\n",
            "\n",
            "\n",
            "Loss:  [0.09788561]\n",
            "Accuracy:  82.264\n",
            "\n",
            "\n",
            "Loss:  [0.09660636]\n",
            "Accuracy:  82.396\n",
            "\n",
            "\n",
            "Loss:  [0.09233467]\n",
            "Accuracy:  82.452\n",
            "\n",
            "\n",
            "Loss:  [0.09070968]\n",
            "Accuracy:  82.564\n",
            "\n",
            "\n",
            "Loss:  [0.09099939]\n",
            "Accuracy:  82.54\n",
            "\n",
            "\n",
            "Loss:  [0.08995059]\n",
            "Accuracy:  82.64\n",
            "\n",
            "\n",
            "Loss:  [0.09041368]\n",
            "Accuracy:  82.724\n",
            "\n",
            "\n",
            "Loss:  [0.0910163]\n",
            "Accuracy:  82.812\n",
            "\n",
            "\n",
            "Loss:  [0.08714274]\n",
            "Accuracy:  82.804\n",
            "\n",
            "\n",
            "Loss:  [0.08305449]\n",
            "Accuracy:  82.832\n",
            "\n",
            "\n",
            "Loss:  [0.0791459]\n",
            "Accuracy:  82.788\n",
            "\n",
            "\n",
            "Loss:  [0.07758337]\n",
            "Accuracy:  82.804\n",
            "\n",
            "\n",
            "Loss:  [0.07804303]\n",
            "Accuracy:  82.812\n",
            "\n",
            "\n",
            "Loss:  [0.08057531]\n",
            "Accuracy:  82.776\n",
            "\n",
            "\n",
            "Loss:  [0.08440978]\n",
            "Accuracy:  82.792\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  81.364\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  5 \n",
            "alpha =  0.0001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69317933]\n",
            "Accuracy:  2.364\n",
            "\n",
            "\n",
            "Loss:  [0.69317402]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "Loss:  [0.69317256]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "Loss:  [0.69317219]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  50.0\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.1 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.03736576]\n",
            "Accuracy:  49.0\n",
            "\n",
            "\n",
            "Loss:  [0.97487967]\n",
            "Accuracy:  48.004\n",
            "\n",
            "\n",
            "Loss:  [0.0413086]\n",
            "Accuracy:  53.396\n",
            "\n",
            "\n",
            "Loss:  [0.00566561]\n",
            "Accuracy:  54.732\n",
            "\n",
            "\n",
            "Loss:  [0.01290012]\n",
            "Accuracy:  59.064\n",
            "\n",
            "\n",
            "Loss:  [1.06656857]\n",
            "Accuracy:  60.384\n",
            "\n",
            "\n",
            "Loss:  [0.01892509]\n",
            "Accuracy:  61.696\n",
            "\n",
            "\n",
            "Loss:  [0.0825529]\n",
            "Accuracy:  61.9\n",
            "\n",
            "\n",
            "Loss:  [0.01473671]\n",
            "Accuracy:  64.608\n",
            "\n",
            "\n",
            "Loss:  [0.37864205]\n",
            "Accuracy:  65.512\n",
            "\n",
            "\n",
            "Loss:  [2.19955759]\n",
            "Accuracy:  65.048\n",
            "\n",
            "\n",
            "Loss:  [0.37680795]\n",
            "Accuracy:  62.16\n",
            "\n",
            "\n",
            "Loss:  [0.66710026]\n",
            "Accuracy:  66.116\n",
            "\n",
            "\n",
            "Loss:  [0.32578797]\n",
            "Accuracy:  64.212\n",
            "\n",
            "\n",
            "Loss:  [0.02807087]\n",
            "Accuracy:  64.912\n",
            "\n",
            "\n",
            "Loss:  [0.52024465]\n",
            "Accuracy:  65.936\n",
            "\n",
            "\n",
            "Loss:  [0.13879725]\n",
            "Accuracy:  64.66\n",
            "\n",
            "\n",
            "Loss:  [0.04647579]\n",
            "Accuracy:  66.616\n",
            "\n",
            "\n",
            "Loss:  [0.10026819]\n",
            "Accuracy:  65.496\n",
            "\n",
            "\n",
            "Loss:  [0.98094989]\n",
            "Accuracy:  63.532\n",
            "\n",
            "\n",
            "Loss:  [0.28900628]\n",
            "Accuracy:  66.868\n",
            "\n",
            "\n",
            "Loss:  [0.91405526]\n",
            "Accuracy:  66.328\n",
            "\n",
            "\n",
            "Loss:  [0.03778595]\n",
            "Accuracy:  64.448\n",
            "\n",
            "\n",
            "Loss:  [0.17809343]\n",
            "Accuracy:  69.916\n",
            "\n",
            "\n",
            "Loss:  [0.43352306]\n",
            "Accuracy:  67.712\n",
            "\n",
            "\n",
            "Loss:  [1.01587521]\n",
            "Accuracy:  70.716\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  76.056\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.05 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.66461037]\n",
            "Accuracy:  58.788\n",
            "\n",
            "\n",
            "Loss:  [0.09032612]\n",
            "Accuracy:  69.944\n",
            "\n",
            "\n",
            "Loss:  [0.21186131]\n",
            "Accuracy:  73.0\n",
            "\n",
            "\n",
            "Loss:  [0.528654]\n",
            "Accuracy:  72.004\n",
            "\n",
            "\n",
            "Loss:  [0.1070653]\n",
            "Accuracy:  72.276\n",
            "\n",
            "\n",
            "Loss:  [0.23455476]\n",
            "Accuracy:  69.524\n",
            "\n",
            "\n",
            "Loss:  [0.211435]\n",
            "Accuracy:  71.716\n",
            "\n",
            "\n",
            "Loss:  [0.48308375]\n",
            "Accuracy:  72.952\n",
            "\n",
            "\n",
            "Loss:  [0.50680489]\n",
            "Accuracy:  71.976\n",
            "\n",
            "\n",
            "Loss:  [0.06704662]\n",
            "Accuracy:  70.964\n",
            "\n",
            "\n",
            "Loss:  [0.34075727]\n",
            "Accuracy:  73.628\n",
            "\n",
            "\n",
            "Loss:  [0.17339798]\n",
            "Accuracy:  72.652\n",
            "\n",
            "\n",
            "Loss:  [0.37658842]\n",
            "Accuracy:  73.924\n",
            "\n",
            "\n",
            "Loss:  [0.31323915]\n",
            "Accuracy:  73.604\n",
            "\n",
            "\n",
            "Loss:  [0.04052485]\n",
            "Accuracy:  74.824\n",
            "\n",
            "\n",
            "Loss:  [0.62363963]\n",
            "Accuracy:  75.432\n",
            "\n",
            "\n",
            "Loss:  [0.60298905]\n",
            "Accuracy:  75.908\n",
            "\n",
            "\n",
            "Loss:  [0.02513357]\n",
            "Accuracy:  76.508\n",
            "\n",
            "\n",
            "Loss:  [0.02040281]\n",
            "Accuracy:  75.396\n",
            "\n",
            "\n",
            "Loss:  [0.0694384]\n",
            "Accuracy:  76.46\n",
            "\n",
            "\n",
            "Loss:  [0.25132115]\n",
            "Accuracy:  75.236\n",
            "\n",
            "\n",
            "Loss:  [0.1878147]\n",
            "Accuracy:  74.924\n",
            "\n",
            "\n",
            "Loss:  [0.167728]\n",
            "Accuracy:  76.444\n",
            "\n",
            "\n",
            "Loss:  [0.02869543]\n",
            "Accuracy:  76.556\n",
            "\n",
            "\n",
            "Loss:  [0.72730558]\n",
            "Accuracy:  75.744\n",
            "\n",
            "\n",
            "Loss:  [0.57329078]\n",
            "Accuracy:  76.432\n",
            "\n",
            "\n",
            "Loss:  [0.24197655]\n",
            "Accuracy:  76.092\n",
            "\n",
            "\n",
            "Loss:  [0.2448112]\n",
            "Accuracy:  75.624\n",
            "\n",
            "\n",
            "Loss:  [0.23328002]\n",
            "Accuracy:  76.104\n",
            "\n",
            "\n",
            "Loss:  [0.18335444]\n",
            "Accuracy:  76.108\n",
            "\n",
            "\n",
            "Loss:  [0.26820807]\n",
            "Accuracy:  77.192\n",
            "\n",
            "\n",
            "Loss:  [0.0623391]\n",
            "Accuracy:  76.304\n",
            "\n",
            "\n",
            "Loss:  [1.23708471]\n",
            "Accuracy:  75.92\n",
            "\n",
            "\n",
            "Loss:  [0.20897087]\n",
            "Accuracy:  77.112\n",
            "\n",
            "\n",
            "Loss:  [0.09524291]\n",
            "Accuracy:  75.816\n",
            "\n",
            "\n",
            "Loss:  [0.10338596]\n",
            "Accuracy:  76.456\n",
            "\n",
            "\n",
            "Loss:  [0.202064]\n",
            "Accuracy:  76.756\n",
            "\n",
            "\n",
            "Loss:  [0.0333166]\n",
            "Accuracy:  77.46\n",
            "\n",
            "\n",
            "Loss:  [0.09456099]\n",
            "Accuracy:  78.084\n",
            "\n",
            "\n",
            "Loss:  [0.40044489]\n",
            "Accuracy:  77.012\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  78.88\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.01 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.20144009]\n",
            "Accuracy:  51.852\n",
            "\n",
            "\n",
            "Loss:  [0.1516377]\n",
            "Accuracy:  76.724\n",
            "\n",
            "\n",
            "Loss:  [0.10772877]\n",
            "Accuracy:  78.288\n",
            "\n",
            "\n",
            "Loss:  [0.1009649]\n",
            "Accuracy:  79.024\n",
            "\n",
            "\n",
            "Loss:  [0.11882704]\n",
            "Accuracy:  79.656\n",
            "\n",
            "\n",
            "Loss:  [0.08044507]\n",
            "Accuracy:  80.312\n",
            "\n",
            "\n",
            "Loss:  [0.13170971]\n",
            "Accuracy:  80.472\n",
            "\n",
            "\n",
            "Loss:  [0.09870306]\n",
            "Accuracy:  80.716\n",
            "\n",
            "\n",
            "Loss:  [0.08497317]\n",
            "Accuracy:  81.02\n",
            "\n",
            "\n",
            "Loss:  [0.06220656]\n",
            "Accuracy:  81.408\n",
            "\n",
            "\n",
            "Loss:  [0.06924614]\n",
            "Accuracy:  81.4\n",
            "\n",
            "\n",
            "Loss:  [0.07812016]\n",
            "Accuracy:  81.464\n",
            "\n",
            "\n",
            "Loss:  [0.06436477]\n",
            "Accuracy:  81.724\n",
            "\n",
            "\n",
            "Loss:  [0.08111468]\n",
            "Accuracy:  81.964\n",
            "\n",
            "\n",
            "Loss:  [0.09104816]\n",
            "Accuracy:  82.1\n",
            "\n",
            "\n",
            "Loss:  [0.06989844]\n",
            "Accuracy:  81.824\n",
            "\n",
            "\n",
            "Loss:  [0.07213265]\n",
            "Accuracy:  82.224\n",
            "\n",
            "\n",
            "Loss:  [0.08871004]\n",
            "Accuracy:  82.364\n",
            "\n",
            "\n",
            "Loss:  [0.072118]\n",
            "Accuracy:  82.372\n",
            "\n",
            "\n",
            "Loss:  [0.05313127]\n",
            "Accuracy:  82.352\n",
            "\n",
            "\n",
            "Loss:  [0.04607966]\n",
            "Accuracy:  82.528\n",
            "\n",
            "\n",
            "Loss:  [0.03806278]\n",
            "Accuracy:  82.72\n",
            "\n",
            "\n",
            "Loss:  [0.03405681]\n",
            "Accuracy:  82.436\n",
            "\n",
            "\n",
            "Loss:  [0.03128806]\n",
            "Accuracy:  82.64\n",
            "\n",
            "\n",
            "Loss:  [0.03911444]\n",
            "Accuracy:  82.628\n",
            "\n",
            "\n",
            "Loss:  [0.05788489]\n",
            "Accuracy:  82.976\n",
            "\n",
            "\n",
            "Loss:  [0.05214472]\n",
            "Accuracy:  82.876\n",
            "\n",
            "\n",
            "Loss:  [0.06775766]\n",
            "Accuracy:  83.0\n",
            "\n",
            "\n",
            "Loss:  [0.07287256]\n",
            "Accuracy:  82.896\n",
            "\n",
            "\n",
            "Loss:  [0.04627911]\n",
            "Accuracy:  83.204\n",
            "\n",
            "\n",
            "Loss:  [0.05568616]\n",
            "Accuracy:  82.892\n",
            "\n",
            "\n",
            "Loss:  [0.06385394]\n",
            "Accuracy:  83.124\n",
            "\n",
            "\n",
            "Loss:  [0.05942215]\n",
            "Accuracy:  83.156\n",
            "\n",
            "\n",
            "Loss:  [0.03125398]\n",
            "Accuracy:  83.548\n",
            "\n",
            "\n",
            "Loss:  [0.06990033]\n",
            "Accuracy:  83.38\n",
            "\n",
            "\n",
            "Loss:  [0.03022146]\n",
            "Accuracy:  83.488\n",
            "\n",
            "\n",
            "Loss:  [0.09976312]\n",
            "Accuracy:  83.384\n",
            "\n",
            "\n",
            "Loss:  [0.08181368]\n",
            "Accuracy:  83.316\n",
            "\n",
            "\n",
            "Loss:  [0.08453961]\n",
            "Accuracy:  83.372\n",
            "\n",
            "\n",
            "Loss:  [0.09039828]\n",
            "Accuracy:  83.732\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  82.932\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.17181809]\n",
            "Accuracy:  37.956\n",
            "\n",
            "\n",
            "Loss:  [0.16137754]\n",
            "Accuracy:  76.92\n",
            "\n",
            "\n",
            "Loss:  [0.12108752]\n",
            "Accuracy:  79.056\n",
            "\n",
            "\n",
            "Loss:  [0.10433112]\n",
            "Accuracy:  79.744\n",
            "\n",
            "\n",
            "Loss:  [0.09654021]\n",
            "Accuracy:  80.496\n",
            "\n",
            "\n",
            "Loss:  [0.08080445]\n",
            "Accuracy:  80.856\n",
            "\n",
            "\n",
            "Loss:  [0.08275681]\n",
            "Accuracy:  81.212\n",
            "\n",
            "\n",
            "Loss:  [0.07844943]\n",
            "Accuracy:  81.524\n",
            "\n",
            "\n",
            "Loss:  [0.07544417]\n",
            "Accuracy:  81.736\n",
            "\n",
            "\n",
            "Loss:  [0.07751502]\n",
            "Accuracy:  81.952\n",
            "\n",
            "\n",
            "Loss:  [0.070499]\n",
            "Accuracy:  82.032\n",
            "\n",
            "\n",
            "Loss:  [0.06776317]\n",
            "Accuracy:  82.292\n",
            "\n",
            "\n",
            "Loss:  [0.06824702]\n",
            "Accuracy:  82.428\n",
            "\n",
            "\n",
            "Loss:  [0.07100926]\n",
            "Accuracy:  82.356\n",
            "\n",
            "\n",
            "Loss:  [0.06306262]\n",
            "Accuracy:  82.56\n",
            "\n",
            "\n",
            "Loss:  [0.06516543]\n",
            "Accuracy:  82.736\n",
            "\n",
            "\n",
            "Loss:  [0.08214639]\n",
            "Accuracy:  82.892\n",
            "\n",
            "\n",
            "Loss:  [0.06898951]\n",
            "Accuracy:  82.856\n",
            "\n",
            "\n",
            "Loss:  [0.08323367]\n",
            "Accuracy:  82.992\n",
            "\n",
            "\n",
            "Loss:  [0.09169267]\n",
            "Accuracy:  82.944\n",
            "\n",
            "\n",
            "Loss:  [0.09540513]\n",
            "Accuracy:  82.784\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  81.132\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69339801]\n",
            "Accuracy:  0.032\n",
            "\n",
            "\n",
            "Loss:  [0.69336809]\n",
            "Accuracy:  19.168\n",
            "\n",
            "\n",
            "Loss:  [0.68321862]\n",
            "Accuracy:  60.32\n",
            "\n",
            "\n",
            "Loss:  [0.10702031]\n",
            "Accuracy:  73.704\n",
            "\n",
            "\n",
            "Loss:  [0.12744717]\n",
            "Accuracy:  79.888\n",
            "\n",
            "\n",
            "Loss:  [0.10919013]\n",
            "Accuracy:  80.664\n",
            "\n",
            "\n",
            "Loss:  [0.09200178]\n",
            "Accuracy:  81.112\n",
            "\n",
            "\n",
            "Loss:  [0.08809127]\n",
            "Accuracy:  81.368\n",
            "\n",
            "\n",
            "Loss:  [0.08446266]\n",
            "Accuracy:  81.528\n",
            "\n",
            "\n",
            "Loss:  [0.07792848]\n",
            "Accuracy:  81.692\n",
            "\n",
            "\n",
            "Loss:  [0.07310052]\n",
            "Accuracy:  81.932\n",
            "\n",
            "\n",
            "Loss:  [0.07002868]\n",
            "Accuracy:  82.056\n",
            "\n",
            "\n",
            "Loss:  [0.06734444]\n",
            "Accuracy:  82.212\n",
            "\n",
            "\n",
            "Loss:  [0.0649942]\n",
            "Accuracy:  82.336\n",
            "\n",
            "\n",
            "Loss:  [0.06239276]\n",
            "Accuracy:  82.428\n",
            "\n",
            "\n",
            "Loss:  [0.05773419]\n",
            "Accuracy:  82.588\n",
            "\n",
            "\n",
            "Loss:  [0.05635143]\n",
            "Accuracy:  82.676\n",
            "\n",
            "\n",
            "Loss:  [0.05506055]\n",
            "Accuracy:  82.692\n",
            "\n",
            "\n",
            "Loss:  [0.05347177]\n",
            "Accuracy:  82.86\n",
            "\n",
            "\n",
            "Loss:  [0.05199]\n",
            "Accuracy:  82.912\n",
            "\n",
            "\n",
            "Loss:  [0.05085611]\n",
            "Accuracy:  82.956\n",
            "\n",
            "\n",
            "Loss:  [0.050046]\n",
            "Accuracy:  82.96\n",
            "\n",
            "\n",
            "Loss:  [0.04961035]\n",
            "Accuracy:  83.028\n",
            "\n",
            "\n",
            "Loss:  [0.04952911]\n",
            "Accuracy:  83.096\n",
            "\n",
            "\n",
            "Loss:  [0.04965976]\n",
            "Accuracy:  83.132\n",
            "\n",
            "\n",
            "Loss:  [0.04987941]\n",
            "Accuracy:  83.132\n",
            "\n",
            "\n",
            "Loss:  [0.0500461]\n",
            "Accuracy:  83.168\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  81.896\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  10 \n",
            "alpha =  0.0001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.6931794]\n",
            "Accuracy:  0.232\n",
            "\n",
            "\n",
            "Loss:  [0.69317492]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "Loss:  [0.69317389]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "Loss:  [0.69317392]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  50.0\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.1 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [2.55024894]\n",
            "Accuracy:  39.468\n",
            "\n",
            "\n",
            "Loss:  [1.62495623]\n",
            "Accuracy:  23.488\n",
            "\n",
            "\n",
            "Loss:  [2.33278824]\n",
            "Accuracy:  31.74\n",
            "\n",
            "\n",
            "Loss:  [1.26774328]\n",
            "Accuracy:  31.568\n",
            "\n",
            "\n",
            "Loss:  [1.47802914]\n",
            "Accuracy:  41.06\n",
            "\n",
            "\n",
            "Loss:  [0.66288329]\n",
            "Accuracy:  44.396\n",
            "\n",
            "\n",
            "Loss:  [2.9997815]\n",
            "Accuracy:  40.352\n",
            "\n",
            "\n",
            "Loss:  [2.32551189]\n",
            "Accuracy:  44.68\n",
            "\n",
            "\n",
            "Loss:  [0.99780098]\n",
            "Accuracy:  47.84\n",
            "\n",
            "\n",
            "Loss:  [3.01553054]\n",
            "Accuracy:  49.524\n",
            "\n",
            "\n",
            "Loss:  [0.23110835]\n",
            "Accuracy:  52.68\n",
            "\n",
            "\n",
            "Loss:  [0.00289806]\n",
            "Accuracy:  49.0\n",
            "\n",
            "\n",
            "Loss:  [1.95930004]\n",
            "Accuracy:  52.368\n",
            "\n",
            "\n",
            "Loss:  [0.11705159]\n",
            "Accuracy:  54.22\n",
            "\n",
            "\n",
            "Loss:  [0.02874826]\n",
            "Accuracy:  54.192\n",
            "\n",
            "\n",
            "Loss:  [0.90877143]\n",
            "Accuracy:  57.952\n",
            "\n",
            "\n",
            "Loss:  [2.81300019]\n",
            "Accuracy:  56.136\n",
            "\n",
            "\n",
            "Loss:  [0.04557855]\n",
            "Accuracy:  54.664\n",
            "\n",
            "\n",
            "Loss:  [0.46001083]\n",
            "Accuracy:  58.52\n",
            "\n",
            "\n",
            "Loss:  [1.48421061]\n",
            "Accuracy:  55.988\n",
            "\n",
            "\n",
            "Loss:  [3.53019974]\n",
            "Accuracy:  58.028\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  65.996\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.05 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.36367766]\n",
            "Accuracy:  58.412\n",
            "\n",
            "\n",
            "Loss:  [0.16935064]\n",
            "Accuracy:  69.424\n",
            "\n",
            "\n",
            "Loss:  [1.04613966]\n",
            "Accuracy:  63.196\n",
            "\n",
            "\n",
            "Loss:  [0.7461356]\n",
            "Accuracy:  55.008\n",
            "\n",
            "\n",
            "Loss:  [0.29658737]\n",
            "Accuracy:  54.584\n",
            "\n",
            "\n",
            "Loss:  [0.40167664]\n",
            "Accuracy:  57.556\n",
            "\n",
            "\n",
            "Loss:  [1.03038617]\n",
            "Accuracy:  52.16\n",
            "\n",
            "\n",
            "Loss:  [1.50497256]\n",
            "Accuracy:  56.092\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  72.34\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.01 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.21625006]\n",
            "Accuracy:  53.132\n",
            "\n",
            "\n",
            "Loss:  [0.17379092]\n",
            "Accuracy:  77.064\n",
            "\n",
            "\n",
            "Loss:  [0.13765306]\n",
            "Accuracy:  78.628\n",
            "\n",
            "\n",
            "Loss:  [0.14266969]\n",
            "Accuracy:  79.456\n",
            "\n",
            "\n",
            "Loss:  [0.11489258]\n",
            "Accuracy:  79.932\n",
            "\n",
            "\n",
            "Loss:  [0.13419879]\n",
            "Accuracy:  79.812\n",
            "\n",
            "\n",
            "Loss:  [0.11510123]\n",
            "Accuracy:  80.172\n",
            "\n",
            "\n",
            "Loss:  [0.11592723]\n",
            "Accuracy:  80.56\n",
            "\n",
            "\n",
            "Loss:  [0.11636559]\n",
            "Accuracy:  81.32\n",
            "\n",
            "\n",
            "Loss:  [0.08286319]\n",
            "Accuracy:  80.904\n",
            "\n",
            "\n",
            "Loss:  [0.07730173]\n",
            "Accuracy:  81.36\n",
            "\n",
            "\n",
            "Loss:  [0.05468052]\n",
            "Accuracy:  81.448\n",
            "\n",
            "\n",
            "Loss:  [0.04204156]\n",
            "Accuracy:  81.724\n",
            "\n",
            "\n",
            "Loss:  [0.13006444]\n",
            "Accuracy:  81.336\n",
            "\n",
            "\n",
            "Loss:  [0.09481201]\n",
            "Accuracy:  81.364\n",
            "\n",
            "\n",
            "Loss:  [0.10090834]\n",
            "Accuracy:  81.624\n",
            "\n",
            "\n",
            "Loss:  [0.07795914]\n",
            "Accuracy:  81.864\n",
            "\n",
            "\n",
            "Loss:  [0.06643898]\n",
            "Accuracy:  81.784\n",
            "\n",
            "\n",
            "Loss:  [0.09980857]\n",
            "Accuracy:  81.756\n",
            "\n",
            "\n",
            "Loss:  [0.13158602]\n",
            "Accuracy:  82.272\n",
            "\n",
            "\n",
            "Loss:  [0.04019361]\n",
            "Accuracy:  82.376\n",
            "\n",
            "\n",
            "Loss:  [0.06728784]\n",
            "Accuracy:  82.6\n",
            "\n",
            "\n",
            "Loss:  [0.10448738]\n",
            "Accuracy:  82.444\n",
            "\n",
            "\n",
            "Loss:  [0.03288701]\n",
            "Accuracy:  82.836\n",
            "\n",
            "\n",
            "Loss:  [0.08377521]\n",
            "Accuracy:  82.728\n",
            "\n",
            "\n",
            "Loss:  [0.1041452]\n",
            "Accuracy:  82.792\n",
            "\n",
            "\n",
            "Loss:  [0.05381469]\n",
            "Accuracy:  82.596\n",
            "\n",
            "\n",
            "Loss:  [0.11764742]\n",
            "Accuracy:  82.48\n",
            "\n",
            "\n",
            "Loss:  [0.02773271]\n",
            "Accuracy:  83.036\n",
            "\n",
            "\n",
            "Loss:  [0.16227925]\n",
            "Accuracy:  83.012\n",
            "\n",
            "\n",
            "Loss:  [0.02857636]\n",
            "Accuracy:  83.124\n",
            "\n",
            "\n",
            "Loss:  [0.03564812]\n",
            "Accuracy:  82.612\n",
            "\n",
            "\n",
            "Loss:  [0.17609572]\n",
            "Accuracy:  82.496\n",
            "\n",
            "\n",
            "Loss:  [0.03206718]\n",
            "Accuracy:  82.868\n",
            "\n",
            "\n",
            "Loss:  [0.12312664]\n",
            "Accuracy:  83.008\n",
            "\n",
            "\n",
            "Loss:  [0.08109437]\n",
            "Accuracy:  82.68\n",
            "\n",
            "\n",
            "Loss:  [0.11341191]\n",
            "Accuracy:  82.804\n",
            "\n",
            "\n",
            "Loss:  [0.14414946]\n",
            "Accuracy:  82.832\n",
            "\n",
            "\n",
            "Loss:  [0.13505167]\n",
            "Accuracy:  82.476\n",
            "\n",
            "\n",
            "Loss:  [0.14998516]\n",
            "Accuracy:  82.84\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  81.216\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.005 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.18717008]\n",
            "Accuracy:  39.82\n",
            "\n",
            "\n",
            "Loss:  [0.15184447]\n",
            "Accuracy:  77.176\n",
            "\n",
            "\n",
            "Loss:  [0.1150694]\n",
            "Accuracy:  79.064\n",
            "\n",
            "\n",
            "Loss:  [0.10627113]\n",
            "Accuracy:  80.168\n",
            "\n",
            "\n",
            "Loss:  [0.08873246]\n",
            "Accuracy:  80.776\n",
            "\n",
            "\n",
            "Loss:  [0.10449849]\n",
            "Accuracy:  81.0\n",
            "\n",
            "\n",
            "Loss:  [0.09568784]\n",
            "Accuracy:  81.512\n",
            "\n",
            "\n",
            "Loss:  [0.10376046]\n",
            "Accuracy:  81.76\n",
            "\n",
            "\n",
            "Loss:  [0.13142568]\n",
            "Accuracy:  81.9\n",
            "\n",
            "\n",
            "Loss:  [0.09723327]\n",
            "Accuracy:  82.092\n",
            "\n",
            "\n",
            "Loss:  [0.12515871]\n",
            "Accuracy:  82.288\n",
            "\n",
            "\n",
            "Loss:  [0.09653931]\n",
            "Accuracy:  82.46\n",
            "\n",
            "\n",
            "Loss:  [0.1090049]\n",
            "Accuracy:  82.44\n",
            "\n",
            "\n",
            "Loss:  [0.07438148]\n",
            "Accuracy:  82.508\n",
            "\n",
            "\n",
            "Loss:  [0.0704576]\n",
            "Accuracy:  82.684\n",
            "\n",
            "\n",
            "Loss:  [0.06760912]\n",
            "Accuracy:  82.748\n",
            "\n",
            "\n",
            "Loss:  [0.06876341]\n",
            "Accuracy:  82.888\n",
            "\n",
            "\n",
            "Loss:  [0.09589681]\n",
            "Accuracy:  83.04\n",
            "\n",
            "\n",
            "Loss:  [0.09529302]\n",
            "Accuracy:  83.128\n",
            "\n",
            "\n",
            "Loss:  [0.09226361]\n",
            "Accuracy:  83.284\n",
            "\n",
            "\n",
            "Loss:  [0.08848661]\n",
            "Accuracy:  83.356\n",
            "\n",
            "\n",
            "Loss:  [0.10735955]\n",
            "Accuracy:  83.496\n",
            "\n",
            "\n",
            "Loss:  [0.0813992]\n",
            "Accuracy:  83.604\n",
            "\n",
            "\n",
            "Loss:  [0.07859626]\n",
            "Accuracy:  83.492\n",
            "\n",
            "\n",
            "Loss:  [0.11608888]\n",
            "Accuracy:  83.512\n",
            "\n",
            "\n",
            "Loss:  [0.07368481]\n",
            "Accuracy:  83.696\n",
            "\n",
            "\n",
            "Loss:  [0.08199222]\n",
            "Accuracy:  83.812\n",
            "\n",
            "\n",
            "Loss:  [0.10003022]\n",
            "Accuracy:  84.036\n",
            "\n",
            "\n",
            "Loss:  [0.07081531]\n",
            "Accuracy:  84.004\n",
            "\n",
            "\n",
            "Loss:  [0.09352589]\n",
            "Accuracy:  83.972\n",
            "\n",
            "\n",
            "Loss:  [0.06560441]\n",
            "Accuracy:  84.08\n",
            "\n",
            "\n",
            "Loss:  [0.07094808]\n",
            "Accuracy:  84.256\n",
            "\n",
            "\n",
            "Loss:  [0.10757355]\n",
            "Accuracy:  84.328\n",
            "\n",
            "\n",
            "Loss:  [0.09084993]\n",
            "Accuracy:  84.568\n",
            "\n",
            "\n",
            "Loss:  [0.09534405]\n",
            "Accuracy:  84.456\n",
            "\n",
            "\n",
            "Loss:  [0.08094825]\n",
            "Accuracy:  84.716\n",
            "\n",
            "\n",
            "Loss:  [0.06156205]\n",
            "Accuracy:  84.824\n",
            "\n",
            "\n",
            "Loss:  [0.06979877]\n",
            "Accuracy:  84.704\n",
            "\n",
            "\n",
            "Loss:  [0.05563189]\n",
            "Accuracy:  84.728\n",
            "\n",
            "\n",
            "Loss:  [0.10202189]\n",
            "Accuracy:  84.836\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  80.868\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69339951]\n",
            "Accuracy:  0.172\n",
            "\n",
            "\n",
            "Loss:  [0.69314543]\n",
            "Accuracy:  37.172\n",
            "\n",
            "\n",
            "Loss:  [0.70840782]\n",
            "Accuracy:  63.212\n",
            "\n",
            "\n",
            "Loss:  [0.12990952]\n",
            "Accuracy:  77.668\n",
            "\n",
            "\n",
            "Loss:  [0.11994396]\n",
            "Accuracy:  80.152\n",
            "\n",
            "\n",
            "Loss:  [0.10059515]\n",
            "Accuracy:  80.784\n",
            "\n",
            "\n",
            "Loss:  [0.08559455]\n",
            "Accuracy:  81.104\n",
            "\n",
            "\n",
            "Loss:  [0.07289509]\n",
            "Accuracy:  81.304\n",
            "\n",
            "\n",
            "Loss:  [0.0667308]\n",
            "Accuracy:  81.568\n",
            "\n",
            "\n",
            "Loss:  [0.06421893]\n",
            "Accuracy:  81.804\n",
            "\n",
            "\n",
            "Loss:  [0.063159]\n",
            "Accuracy:  81.912\n",
            "\n",
            "\n",
            "Loss:  [0.06236537]\n",
            "Accuracy:  82.06\n",
            "\n",
            "\n",
            "Loss:  [0.06143271]\n",
            "Accuracy:  82.244\n",
            "\n",
            "\n",
            "Loss:  [0.05563829]\n",
            "Accuracy:  82.388\n",
            "\n",
            "\n",
            "Loss:  [0.05381423]\n",
            "Accuracy:  82.556\n",
            "\n",
            "\n",
            "Loss:  [0.05406359]\n",
            "Accuracy:  82.66\n",
            "\n",
            "\n",
            "Loss:  [0.05494379]\n",
            "Accuracy:  82.776\n",
            "\n",
            "\n",
            "Loss:  [0.0541068]\n",
            "Accuracy:  82.796\n",
            "\n",
            "\n",
            "Loss:  [0.05267709]\n",
            "Accuracy:  82.912\n",
            "\n",
            "\n",
            "Loss:  [0.05105654]\n",
            "Accuracy:  82.996\n",
            "\n",
            "\n",
            "Loss:  [0.04940253]\n",
            "Accuracy:  83.044\n",
            "\n",
            "\n",
            "Loss:  [0.04790242]\n",
            "Accuracy:  83.116\n",
            "\n",
            "\n",
            "Loss:  [0.04671888]\n",
            "Accuracy:  83.192\n",
            "\n",
            "\n",
            "Loss:  [0.04553568]\n",
            "Accuracy:  83.24\n",
            "\n",
            "\n",
            "Loss:  [0.04432981]\n",
            "Accuracy:  83.296\n",
            "\n",
            "\n",
            "Loss:  [0.04319218]\n",
            "Accuracy:  83.284\n",
            "\n",
            "\n",
            "Loss:  [0.04217735]\n",
            "Accuracy:  83.376\n",
            "\n",
            "\n",
            "Loss:  [0.0413226]\n",
            "Accuracy:  83.504\n",
            "\n",
            "\n",
            "Loss:  [0.04058325]\n",
            "Accuracy:  83.536\n",
            "\n",
            "\n",
            "Loss:  [0.03994001]\n",
            "Accuracy:  83.504\n",
            "\n",
            "\n",
            "Loss:  [0.0394277]\n",
            "Accuracy:  83.52\n",
            "\n",
            "\n",
            "Loss:  [0.03901987]\n",
            "Accuracy:  83.532\n",
            "\n",
            "\n",
            "Loss:  [0.03868478]\n",
            "Accuracy:  83.584\n",
            "\n",
            "\n",
            "Loss:  [0.03844594]\n",
            "Accuracy:  83.608\n",
            "\n",
            "\n",
            "Loss:  [0.03827788]\n",
            "Accuracy:  83.628\n",
            "\n",
            "\n",
            "Loss:  [0.0381816]\n",
            "Accuracy:  83.656\n",
            "\n",
            "\n",
            "Loss:  [0.03798672]\n",
            "Accuracy:  83.712\n",
            "\n",
            "\n",
            "Loss:  [0.03786035]\n",
            "Accuracy:  83.7\n",
            "\n",
            "\n",
            "Loss:  [0.03787609]\n",
            "Accuracy:  83.704\n",
            "\n",
            "\n",
            "Loss:  [0.03796921]\n",
            "Accuracy:  83.616\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  82.228\n",
            "\n",
            " _________________________________________________________ \n",
            "\n",
            "h =  20 \n",
            "alpha =  0.0001 \n",
            "\n",
            "Training has begun...\n",
            "\n",
            "Loss:  [0.69317961]\n",
            "Accuracy:  3.7\n",
            "\n",
            "\n",
            "Loss:  [0.69317375]\n",
            "Accuracy:  0.0\n",
            "\n",
            "\n",
            "Loss:  [0.6931718]\n",
            "Accuracy:  0.012\n",
            "\n",
            "\n",
            "Loss:  [0.69317097]\n",
            "Accuracy:  0.284\n",
            "\n",
            "\n",
            "\n",
            "Testing has begun...\n",
            "\n",
            "Test Accuracy =  50.148\n",
            "\n",
            " _________________________________________________________ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOJT7hCjcysG",
        "colab_type": "text"
      },
      "source": [
        "If you scroll down in the above outputs you'll find that for h = 10 and learning rate = 0.01 we obtain a training accuracy of 83.72% and a test accuracy of 82.93%. Thus, surprisingly, this method works better for the IMDB dataset somehow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPiIYTkZftvo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLqwbba1fvEB",
        "colab_type": "text"
      },
      "source": [
        "##Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzTXmfySfxph",
        "colab_type": "text"
      },
      "source": [
        "**We have mostly achieved a training and test accuracy of around 80%. Highest being 83.7% and 82.9% respectively.** \n",
        "\n",
        "According to me, I've tried on several combinations of hyperparameters, but there could still be some which might lead to increase in accuracy. For inputs, GloVe led to the best accuracy. Passing the one-hot vectors of words or characters, or taking the one-hot vectors of the shortened vocabulary were either leading to low accuracy or were infeasable due to obvious reasons. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpelJr27hLRY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RXSIPB_ezHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}